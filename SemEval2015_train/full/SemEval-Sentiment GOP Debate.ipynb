{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 13871 rows, 21 columns\n",
      "    id               candidate  candidate_confidence relevant_yn  \\\n",
      "0    1  No candidate mentioned                     1         yes   \n",
      "1    2            Scott Walker                     1         yes   \n",
      "2    3  No candidate mentioned                     1         yes   \n",
      "3    4  No candidate mentioned                     1         yes   \n",
      "4    5            Donald Trump                     1         yes   \n",
      "\n",
      "   relevant_yn_confidence sentiment  sentiment_confidence     subject_matter  \\\n",
      "0                       1   Neutral                0.6578  None of the above   \n",
      "1                       1  Positive                0.6333  None of the above   \n",
      "2                       1   Neutral                0.6629  None of the above   \n",
      "3                       1  Positive                1.0000  None of the above   \n",
      "4                       1  Positive                0.7045  None of the above   \n",
      "\n",
      "   subject_matter_confidence candidate_gold             ...              \\\n",
      "0                     1.0000            NaN             ...               \n",
      "1                     1.0000            NaN             ...               \n",
      "2                     0.6629            NaN             ...               \n",
      "3                     0.7039            NaN             ...               \n",
      "4                     1.0000            NaN             ...               \n",
      "\n",
      "  relevant_yn_gold retweet_count  sentiment_gold subject_matter_gold  \\\n",
      "0              NaN             5             NaN                 NaN   \n",
      "1              NaN            26             NaN                 NaN   \n",
      "2              NaN            27             NaN                 NaN   \n",
      "3              NaN           138             NaN                 NaN   \n",
      "4              NaN           156             NaN                 NaN   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0  RT @NancyLeeGrahn: How did everyone feel about...         NaN   \n",
      "1  RT @ScottWalker: Didn't catch the full #GOPdeb...         NaN   \n",
      "2  Re-SubmissionT @TJMShow: No mention of Tamir R...         NaN   \n",
      "3  RT @RobGeorge: That Carly Fiorina is trending ...         NaN   \n",
      "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...         NaN   \n",
      "\n",
      "               tweet_created            tweet_id  tweet_location  \\\n",
      "0  2015-08-07 09:54:46 -0700  629697200650592256             NaN   \n",
      "1  2015-08-07 09:54:46 -0700  629697199560069120             NaN   \n",
      "2  2015-08-07 09:54:46 -0700  629697199312482304             NaN   \n",
      "3  2015-08-07 09:54:45 -0700  629697197118861312           Texas   \n",
      "4  2015-08-07 09:54:45 -0700  629697196967903232             NaN   \n",
      "\n",
      "                user_timezone  \n",
      "0                       Quito  \n",
      "1                         NaN  \n",
      "2                         NaN  \n",
      "3  Central Time (US & Canada)  \n",
      "4                     Arizona  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('/Users/Amar/PycharmProjects/MLND_projects/Capstone/data/first-GOP-debate/Sentiment.csv')\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "print data.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' id', 0), ('candidate', 1), ('candidate_confidence', 2), ('relevant_yn', 3), ('relevant_yn_confidence', 4), ('sentiment', 5), ('sentiment_confidence', 6), ('subject_matter', 7), ('subject_matter_confidence', 8), ('candidate_gold', 9), ('name', 10), ('relevant_yn_gold', 11), ('retweet_count', 12), ('sentiment_gold', 13), ('subject_matter_gold', 14), ('text', 15), ('tweet_coord', 16), ('tweet_created', 17), ('tweet_id', 18), ('tweet_location', 19), ('user_timezone', 20)]\n"
     ]
    }
   ],
   "source": [
    "#A lot of information we really aren't interested in, so take a look at the columns with their index.\n",
    "header_index = [(i,z) for z,i in enumerate(data.columns.view())]\n",
    "print header_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                               text\n",
      "0   Neutral  RT @NancyLeeGrahn: How did everyone feel about...\n",
      "1  Positive  RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
      "2   Neutral  Re-SubmissionT @TJMShow: No mention of Tamir R...\n",
      "3  Positive  RT @RobGeorge: That Carly Fiorina is trending ...\n",
      "4  Positive  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...\n"
     ]
    }
   ],
   "source": [
    "#Now we can drop the ones we won't be using.  Keeping ID-0, sentiment-5, sentiment_confidence-6, text-15.\n",
    "df = data.drop(data.columns[[0,1,2,3,4,6,7,8,9,10,11,12,13,14,16,17,18,19,20]], axis=1) #probably should keep sentiment confidance\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentiment                                               text\n",
      "0           0  RT @NancyLeeGrahn: How did everyone feel about...\n",
      "1           1  RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
      "2           0  Re-SubmissionT @TJMShow: No mention of Tamir R...\n",
      "3           1  RT @RobGeorge: That Carly Fiorina is trending ...\n",
      "4           1  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...\n",
      "5           1  RT @GregAbbott_TX: @TedCruz: \"On my first day ...\n",
      "6           0  RT @warriorwoman91: I liked her and was happy ...\n",
      "7           0  Going on #MSNBC Live with @ThomasARoberts arou...\n",
      "8           0  Deer in the headlights RT @lizzwinstead: Ben C...\n",
      "9           0  RT @NancyOsborne180: Last night's debate prove...\n",
      "10          0  @JGreenDC @realDonaldTrump In all fairness #Bi...\n",
      "11          1  RT @WayneDupreeShow: Just woke up to tweet thi...\n",
      "12          0  Me reading my family's comments about how grea...\n",
      "13          0  RT @ArcticFox2016: RT @AllenWestRepub \"Dear @J...\n",
      "14          1  RT @pattonoswalt: I loved Scott Walker as Mark...\n",
      "15          0  Hey @ChrisChristie exploiting the tragedy of 9...\n",
      "16          0  RT @CarolCNN: #DonaldTrump under fire for comm...\n",
      "17          0  RT @johncardillo: Guess who had most speaking ...\n",
      "18          0  reason comment is funny 'in case you're ignora...\n",
      "19          0  RT @PamelaGeller: Huckabee: Paying for transge...\n"
     ]
    }
   ],
   "source": [
    "#Next we need to re-write the neutral / objective labels to all be neutral.  The organizers kept this distinction\n",
    "#for other tasks, but for this task, it's considered the same.\n",
    "# so, let's re-write all objective ->neutral, and all neutral-OR-objective --> neutral.\n",
    "\n",
    "#Since we probably will want our labels numeric (some classifiers may not like 3-way text labels),\n",
    "#we can do that all now.\n",
    "\n",
    "df = df.apply(lambda x: x.replace(['Positive', 'Negative', 'Neutral'] # let's just do positive first\n",
    "                                  , [1, 0,0]) ,1)\n",
    "print df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples is : 13871\n",
      "There are 2236 positive tweets or 0.161199625117%\n",
      "There are 0 Negative tweets or 0.0%\n",
      "There are 11635 Neutral tweets or 0.838800374883%\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at our class distribution\n",
    "total_tweets = len(df)\n",
    "positive_tweets = sum(df.sentiment == 1)\n",
    "negative_tweets = sum(df.sentiment == -1)\n",
    "neutral_tweets = sum(df.sentiment == 0)\n",
    "\n",
    "print \"The total number of samples is : {}\".format(len(df.sentiment))\n",
    "print \"There are {} positive tweets or {}%\".format \\\n",
    "(positive_tweets, positive_tweets/float(total_tweets) )\n",
    "print \"There are {} Negative tweets or {}%\".format \\\n",
    "(negative_tweets, negative_tweets / float(total_tweets))\n",
    "print \"There are {} Neutral tweets or {}%\".format \\\n",
    "(neutral_tweets, neutral_tweets/ float(total_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$3.39!!!!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat.', ':)']\n"
     ]
    }
   ],
   "source": [
    "# Let's load the texts into lists and remove RT's and URls\n",
    "# we will build a custom function for an individual tweet, \n",
    "#and then use Pandas Dataframe.apply() to run it on all tweets.\n",
    "\n",
    "first_tweet = \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "def parse_tweet (text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "    \n",
    "parsed_tweet = parse_tweet(first_tweet)\n",
    "print parsed_tweet\n",
    "#this results in the most basic splitting operation.  However it gets us very close to what we want.\n",
    "#In the below output the only concern I have is with \"!!!!\" attached to \"$3.39\".  This is not really ideal.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours\\tourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'I', '\\\\.', 'The', '\\\\.\\\\.']\n"
     ]
    }
   ],
   "source": [
    "#stop-word removal\n",
    "with open('stopwords.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "    stop_words.extend(['I', '\\.', 'The','\\.\\.']) #add upper-case I\n",
    "print stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'house', 'hit', '$339', '!', '!', '!', '!', \"'m\", 'going', 'Chapel', 'Hill', 'Sat', '.', ':)']\n",
      "['Cool', '#cdnpoli', 'Call', 'hospital', 'Iqaluit', '&amp;', 'press', '2', 'English', '.', 'Experience', 'aboriginal', 'language', '1st', 'choice']\n",
      "['For', 'long,', 'might', 'NJ', '?', '@blove402', 'Thursday', 'Night', '13th', 'Dec', '.']\n",
      "['Get', 'ready', 'Wednesday', 'Drink', 'Specials', 'Wednesday', '-', '3-8pm', 'Have', 'Way', 'Margarita', 'Day', '(', 'Bar', 'Brand', 'Only)', 'DOTDOTDOT']\n"
     ]
    }
   ],
   "source": [
    "#Let's enhance the parser to deal with a few more special cases\n",
    "\n",
    "#compile regex outside of the function, because we will be running this function in a loop.\n",
    "retweets = re.compile(r'(RT ?@.*?:)')   \n",
    "urls = re.compile(r'(https?:.*\\b)')\n",
    "dotdotdot = re.compile(r'(\\.\\.\\.)')\n",
    "pound_question = re.compile(r'([!\\?])')\n",
    "period_dot = re.compile(r'(\\.(?!\\d))')\n",
    "stop_word = re.compile(r'\\b(?:{})\\b'.format('|'.join(stop_words)))\n",
    "\n",
    "regex_args = (retweets, urls, dotdotdot, pound_question, period_dot, stop_word)\n",
    "\n",
    "def parse_tweet (text , retweets, urls, dotdotdot, pound_question, period_dot, stop_word):\n",
    "    text = re.sub(retweets, \"\", text) #removes RT@thisguy: or RT @thisguy:   two common Retweet bits I dont' need\n",
    "    text = re.sub(urls, \"\", text) # removes URL's\n",
    "    text = re.sub(dotdotdot, ' DOTDOTDOT ', text) #replace '...' with \"DOTDOTDOT' so i preserve the meaning in that token\n",
    "    text = re.sub(pound_question, r' \\1 ', text)  #eyes bleeding? Searches for ! ? and adds white space around them.\n",
    "    text = re.sub(period_dot, r' \\1 ', text) #more blood.  searched for '.' but looks ahead for digits. will not break 3.39\n",
    "    text = re.sub(stop_word, \"\", text) #removes stop words.\n",
    "    \n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "############\n",
    "#Test cases#\n",
    "############\n",
    "first_tweet = \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "RT_tweet_1 = \"Cool #cdnpoli RT@angelpike: Call the hospital in Iqaluit &amp; press 2 for English. \\\n",
    "Experience an aboriginal language as 1st choice\"\n",
    "RT_tweet_2 = \"For how long, i might be in NJ then?RT @FoolishInApril: @blove402 Thursday Night the 13th of Dec.\"\n",
    "URL_tweet = \"Get ready for our Wednesday Drink Specials Wednesday - 3-8pm Have it your Way Margarita Day \\\n",
    "( Bar Brand Only)... http://t.co/ml806WRT\"\n",
    "\n",
    "test1 = parse_tweet(first_tweet, *regex_args)\n",
    "test2 = parse_tweet(RT_tweet_1, *regex_args)\n",
    "test3 = parse_tweet(RT_tweet_2, *regex_args)\n",
    "test4 = parse_tweet(URL_tweet, *regex_args)\n",
    "print test1\n",
    "print test2\n",
    "print test3\n",
    "print test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                               text\n",
      "0          0  [How, everyone, feel, Climate, Change, questio...\n",
      "1          1  [Didn't, catch, full, #GOPdebate, last, night,...\n",
      "2          0  [Re-SubmissionT, @TJMShow:, No, mention, Tamir...\n",
      "3          1  [That, Carly, Fiorina, trending, --, hours, HE...\n",
      "4          1  [#GOPDebate, w/, @realDonaldTrump, delivered, ...\n",
      "(13871, 2)\n"
     ]
    }
   ],
   "source": [
    "# ok, now that we have rough parsing, lets parse them all!\n",
    "df.text = df.text.apply(lambda x: parse_tweet(x,*regex_args))\n",
    "print df.head()\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13857, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop some tweets that got parsed to zero\n",
    "df = df[df['text'].map(len) >= 1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training tweets:  10392\n",
      "size of testing tweets:  3465\n"
     ]
    }
   ],
   "source": [
    "# now that we have all the tweets parsed, we actually want to split into our training / testing sets. \n",
    "# This is because n-gram analysis (which comes next), should not be done on the testing data!  \n",
    "# The n-gram analysis should on be on training data.  \n",
    "\n",
    "# TODO try to implement n-gram analysis with cross validation, for now I'll use a hold-out testing set\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#Let's split up the labels from the training data\n",
    "\n",
    "X_all = df['text']\n",
    "y_all = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "        X_all, y_all, test_size=0.25, stratify = y_all)\n",
    "\n",
    "print \"size of training tweets: \", len(X_train)\n",
    "print \"size of testing tweets: \", len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  sentiment\n",
      "11860  [You, never, know, @realDonaldTrump, frontrunn...          1\n",
      "1865   [My, takeaway, Mike, Huckabee's, going, privat...          0\n",
      "1354   [Here's, Donald, Trump, channelling, inner, c*...          0\n",
      "3641   [My, two, cents:, What,, anything,, learn, las...          0\n",
      "3421   [#FF, Best, source, 's, going, US, Senate:, @S...          1\n",
      "11860    [You, never, know, @realDonaldTrump, frontrunn...\n",
      "1865     [My, takeaway, Mike, Huckabee's, going, privat...\n",
      "1354     [Here's, Donald, Trump, channelling, inner, c*...\n",
      "3641     [My, two, cents:, What,, anything,, learn, las...\n",
      "3421     [#FF, Best, source, 's, going, US, Senate:, @S...\n",
      "12107    [Jebimiah, talking, respect, life, ., GOP, car...\n",
      "13748    [\"God, loves, needs, money\", George, Carlin, &...\n",
      "772      [As, watch, #GOPDebate,, pray, future, nation,...\n",
      "5031     [Well,, 's, ., And, minds, aliens, watching, o...\n",
      "13416    [#GOPDebates, Stop, secular, progressive, move...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# tricked you! we need to merge the labels and parsed tweets for doing our n-gram analysis.\n",
    "# This is because we will build n-gram models for each class, therefore we need to select only\n",
    "# those tweets that are positive / negative for the two n-gram tables.\n",
    "\n",
    "# Let's re-merge the labels into the training data order to do n-gram analysis\n",
    "XyN_gram = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "print XyN_gram.head()\n",
    "print XyN_gram.text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('.',): 1, (\"'m\",): 1, ('Hill',): 1, ('!',): 4, ('$339',): 1, ('Sat',): 1, ('house',): 1, ('hit',): 1, ('Chapel',): 1, ('Gas',): 1, ('going',): 1}\n",
      " \n",
      "{('going', 'Chapel'): 1, ('!', '!'): 3, ('!', \"'m\"): 1, (\"'m\", 'going'): 1, ('Sat', '.'): 1, ('hit', '$339'): 1, ('Hill', 'Sat'): 1, ('$339', '!'): 1, ('house', 'hit'): 1, ('Chapel', 'Hill'): 1, ('Gas', 'house'): 1}\n",
      " \n",
      "{('!', \"'m\", 'going'): 1, ('Chapel', 'Hill', 'Sat'): 1, ('house', 'hit', '$339'): 1, ('Hill', 'Sat', '.'): 1, ('!', '!', \"'m\"): 1, (\"'m\", 'going', 'Chapel'): 1, ('hit', '$339', '!'): 1, ('Gas', 'house', 'hit'): 1, ('!', '!', '!'): 2, ('$339', '!', '!'): 1, ('going', 'Chapel', 'Hill'): 1}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Let's start by developing a function that will take a parsed tweet and output grams of any size\n",
    "\n",
    "uni_gram_map = {}\n",
    "bi_gram_map = {}\n",
    "tri_gram_map = {}\n",
    "\n",
    "def nGram_counter (parsed_tweet, distance_to_cover, gram_map):\n",
    "    for_loop_range = range(len(parsed_tweet) - distance_to_cover)    \n",
    "    for i in for_loop_range:\n",
    "        gram = tuple(parsed_tweet[i:i+distance_to_cover])\n",
    "        if gram in gram_map:\n",
    "            gram_map[gram] += 1\n",
    "        else:\n",
    "            gram_map[gram] = 1\n",
    "\n",
    "nGram_counter(test1, 3, tri_gram_map)\n",
    "nGram_counter(test1, 2, bi_gram_map)\n",
    "nGram_counter(test1, 1, uni_gram_map)\n",
    "\n",
    "#Our output should be a dictionaries of all possible tri-grams, bi-grams and unigrams of the tweet\n",
    "#\"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "\n",
    "# if a particular gram exists more than once in a tweet, the counter should have incremented.  We see an example of this\n",
    "# with the token \"!!!!\" which is parsed into \"!!\" 3 times. and \"!!!\" twice. (it overlaps).\n",
    "\n",
    "\n",
    "# check our output.\n",
    "print uni_gram_map\n",
    "print \" \"\n",
    "print bi_gram_map\n",
    "print \" \"\n",
    "print tri_gram_map\n",
    "print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unigrams for Positive Tweets : 4448\n",
      "\n",
      "Total Bi-grams for Positive Tweets: 10281\n",
      "\n",
      "Total Tri-grams for Positive Tweets: 10525\n",
      "\n",
      "Most popular Positive Uni-grams : [(('.',), 1289), (('#GOPDebate',), 639), (('!',), 374), (('Trump',), 266), (('\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8',), 212), (('@realDonaldTrump',), 210), (('#GOPDebates',), 206), (('&amp;',), 126), ((\"'s\",), 116), (('?',), 113), (('last',), 112), (('need',), 95), (('Cruz',), 92), (('think',), 91), (('debate',), 88), (('Fox',), 85), (('will',), 84), (('get',), 84), (('night',), 78), (('\"',), 75), (('said',), 71), (('Thanks',), 71), (('DOTDOTDOT',), 67), (('like',), 66), (('#TedCruz',), 65), (('Bush',), 65), (('candidates',), 64), (('@megynkelly',), 64), (('next',), 63), (('ratings',), 63)]\n",
      "\n",
      "Most Popular Positive  Bi-gams : [(('\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8', '#GOPDebate'), 186), (('.', '\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8'), 113), (('!', '!'), 88), (('.', '#GOPDebate'), 69), (('get', 'rid'), 57), (('Cruz', 'Trump'), 57), (('together', 'expose'), 56), (('job,', 'get'), 56), (('rid', 'Bush'), 56), (('Trump', 'need'), 56), (('expose', 'set'), 56), (('band', 'together'), 56), (('set', 'job,'), 56), (('need', 'band'), 56), (('Bush', 'Rubio,'), 56), (('think', 'Cruz'), 56), (('Rubio,', '\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8'), 56), (('last', 'night'), 55), (('@realDonaldTrump', \"'s\"), 54), ((\"'s\", 'ratings'), 52), (('raising', '@realDonaldTrump'), 49), (('Thanks', 'Fox'), 49), (('News,', \"'re\"), 49), ((\"'re\", 'raising'), 49), (('ratings', '.'), 49), (('Fox', 'News,'), 49), (('.', '\"'), 47), (('looking', 'forward'), 46), (('#DemocraticDebates', 'next'), 45), (('Enjoyed', '#GOPDebates'), 45)]\n",
      "\n",
      "Most popular Positive Tri-grams : [(('.', '\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8', '#GOPDebate'), 106), (('set', 'job,', 'get'), 56), (('together', 'expose', 'set'), 56), (('rid', 'Bush', 'Rubio,'), 56), (('need', 'band', 'together'), 56), (('Bush', 'Rubio,', '\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8'), 56), (('band', 'together', 'expose'), 56), (('think', 'Cruz', 'Trump'), 56), (('Cruz', 'Trump', 'need'), 56), (('job,', 'get', 'rid'), 56), (('expose', 'set', 'job,'), 56), (('Trump', 'need', 'band'), 56), (('get', 'rid', 'Bush'), 56), (('Rubio,', '\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8', '#GOPDebate'), 56), (('@realDonaldTrump', \"'s\", 'ratings'), 52), (('raising', '@realDonaldTrump', \"'s\"), 49), ((\"'re\", 'raising', '@realDonaldTrump'), 49), (('News,', \"'re\", 'raising'), 49), (('Thanks', 'Fox', 'News,'), 49), (('ratings', '.', '\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8'), 49), (('Fox', 'News,', \"'re\"), 49), ((\"'s\", 'ratings', '.'), 49), (('looking', 'forward', '#DemocraticDebates'), 45), (('forward', '#DemocraticDebates', 'next'), 45), (('#GOPDebates', 'looking', 'forward'), 45), (('Enjoyed', '#GOPDebates', 'looking'), 45), (('.', 'Look', \"Wallace's\"), 38), (('question', '.', 'Look'), 38), ((\"Wallace's\", 'face', 'Trump'), 38), (('nails', '.', '\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8'), 38)]\n"
     ]
    }
   ],
   "source": [
    "#let's now apply our n-gram counter to all the tweets of a certain class.\n",
    "# Let's make the positive n_gram map, on the training data.\n",
    "\n",
    "#for now I will merge all grams into a single map, maybe harder for stats later, but easier for coding now\n",
    "\n",
    "#setup n-gram maps.\n",
    "pos_uni_gram_map ={}\n",
    "pos_bi_gram_map = {}\n",
    "pos_tri_gram_map = {}\n",
    "\n",
    "pos_tweets = XyN_gram[XyN_gram.sentiment == 1]\n",
    "\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 1, pos_uni_gram_map), 1)\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 2, pos_bi_gram_map), 1)\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 3, pos_tri_gram_map), 1)\n",
    "print \"Total Unigrams for Positive Tweets : {}\".format(len(pos_uni_gram_map))\n",
    "print \n",
    "print \"Total Bi-grams for Positive Tweets: {}\".format(len(pos_bi_gram_map))\n",
    "print\n",
    "print \"Total Tri-grams for Positive Tweets: {}\".format(len(pos_tri_gram_map))\n",
    "print\n",
    "print \"Most popular Positive Uni-grams : {}\" \\\n",
    ".format(sorted(pos_uni_gram_map.items(), key=lambda x: x[1], reverse = True)[:30])\n",
    "print\n",
    "print \"Most Popular Positive  Bi-gams : {}\" \\\n",
    ".format(sorted(pos_bi_gram_map.items(), key = lambda x: x[1], reverse = True)[:30])\n",
    "print\n",
    "print \"Most popular Positive Tri-grams : {}\" \\\n",
    ".format(sorted(pos_tri_gram_map.items(), key=lambda x: x[1], reverse = True)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ok, lets use these gram maps to create some features finally.\n",
    "so what we want to do is :\n",
    "\n",
    "take each tweet.text and calculate the probability of that tweet existing as a positive tweet.  \n",
    "we can use this feature to construct our first classifier, for positive tweets.\n",
    "let's start by defining a function that calculates the probability of a tweet.  \n",
    "I will need to include smoothing, normalization and worry about over / underflow.\n",
    "actually the very first step is to transform our maps into maximum likliehood probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "[0.00011041183614883516, 0.00011021105416873313, 0.0001655263738689031, 0.0005517545795630103, 0.00010918818583829229, 0.0001104484205875856, 0.00011034482758620689, 0.00016566348224639682, 0.00010990218705352236, 0.00011024750565018466]\n"
     ]
    }
   ],
   "source": [
    "# maximum likliehood probabilities for positive grams.\n",
    "# we will calculate maximum likliehood with smoothing, will use simple k-smoothing, with k = 1\n",
    "\n",
    "\n",
    "def calculate_maximum_likliehood (gram_map, k_smoothing = 1, Prior_map = None):\n",
    "    MLE_estimates = {}\n",
    "    total_unique_grams = len(gram_map) # this is V for smoothing \n",
    "    total_gram_count = sum(gram_map.values())\n",
    "    \n",
    "    if Prior_map != None:\n",
    "        total_prior_gram_count = sum(Prior_map.values()) # also V for smoothing on conditioned grams\n",
    "    \n",
    "    #figure out what kind of gram-map we have\n",
    "    keys = gram_map.keys()\n",
    "    if len(keys[0]) == 1: # we have unigrams\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key]+ k_smoothing) / \\\n",
    "            float(total_unique_grams + k_smoothing * total_gram_count)\n",
    "            # above will give MLE with smoothing = 1\n",
    "                \n",
    "    elif len(keys[0]) == 2: # This means we want to condition on previous uni gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float(Prior_map[key[0],] + k_smoothing * total_prior_gram_count)\n",
    "    else: #should be 3 size, so condition on previous bi-gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float(Prior_map[key[:2]] + k_smoothing * total_prior_gram_count)\n",
    "            \n",
    "\n",
    "    return MLE_estimates\n",
    "\n",
    "MLE_pos_uni_gram = calculate_maximum_likliehood(pos_uni_gram_map , 1)\n",
    "MLE_pos_bi_gram = calculate_maximum_likliehood(pos_bi_gram_map, 1, Prior_map=pos_uni_gram_map)\n",
    "MLE_pos_tri_gram = calculate_maximum_likliehood(pos_tri_gram_map, 1, Prior_map=pos_bi_gram_map)\n",
    "\n",
    "## sanity checks\n",
    "print len(MLE_pos_uni_gram) == len(pos_uni_gram_map)\n",
    "print len(MLE_pos_bi_gram) == len(pos_bi_gram_map)\n",
    "print len(MLE_pos_tri_gram) == len(pos_tri_gram_map)\n",
    "\n",
    "# should look reasonble?\n",
    "print MLE_pos_bi_gram.values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ok, now we have MLE for all the training data.  This is n-gram analysis on the positive data.\n",
    "NExt we need to reparse all the tweets, looking up their values in the MLE_pos gram maps.\n",
    "Take log probabilities of everything If a gram doesn't exist in the correct place, then we'll use smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.94144152246e-65\n",
      "1.24298450991e-68\n",
      "1.17300230295e-68\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def v_plus_n(grams):\n",
    "    total_unique_grams = len(grams) # this is V for smoothing \n",
    "    total_gram_count = sum(grams.values()) #this is N\n",
    "    return float(total_unique_grams + total_gram_count)\n",
    "\n",
    "\n",
    "def positive_probability_calculator (parsed_tweet, gram_size):\n",
    "    \n",
    "    if len(parsed_tweet) <1:  #this will catch any empty tweets I missed earlier.\n",
    "        return \"NaN\"\n",
    "    \n",
    "    # access the gram maps we've calculated before\n",
    "    global MLE_pos_uni_gram\n",
    "    global MLE_pos_bi_gram\n",
    "    global MLE_pos_tri_gram\n",
    "    \n",
    "    global pos_uni_gram_map\n",
    "    global pos_bi_gram_map\n",
    "    \n",
    "    uni_VplusN = v_plus_n(pos_uni_gram_map) # will use these values in smoothing\n",
    "    bi_VplusN = v_plus_n(pos_bi_gram_map)\n",
    "    tri_VplusN = v_plus_n(pos_tri_gram_map)\n",
    "        \n",
    "    # gram_map should correspond to gram_size i.E bi-grams, or tri-grams etc.\n",
    "    loop_range = range(len(parsed_tweet) - gram_size)\n",
    "    prob = 0\n",
    "    \n",
    "    if gram_size == 1: #unigrams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_uni_gram: #look up the probability value we've already calculated\n",
    "                prob += math.log(MLE_pos_uni_gram[gram])\n",
    "            else:  #it's unseen so create a new probability with k-smoothing\n",
    "                #pass # penalize it with nothing\n",
    "                prob += math.log( 1.0 / uni_VplusN )  \n",
    "    \n",
    "    if gram_size == 2: #bi-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_bi_gram:\n",
    "                prob += math.log(MLE_pos_bi_gram[gram])  #look up probability we've calculated\n",
    "            \n",
    "            else:  #condition the unseen bi-gram on the seen unigram.\n",
    "                #pass\n",
    "                if (gram[0],) in pos_uni_gram_map:\n",
    "                    prob += math.log( 1.0 / (pos_uni_gram_map[gram[0],] + len(pos_uni_gram_map)))  \n",
    "                    \n",
    "                    # so if gram = ('this','cat'), and we have never seen that before.  we are\n",
    "                    # getting a probability that is: 1 / count('this') + count(unique_single grams)\n",
    "                    #obviously close to zero.  ....\n",
    "                else: #then even the first part of this unseen bigram is not the unigram database, just do V+N\n",
    "                    prob += math.log(1.0 / bi_VplusN)\n",
    "    \n",
    "    if gram_size == 3: #tri-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_tri_gram:\n",
    "                prob += math.log(MLE_pos_tri_gram[gram]) # look up prob we've already calculated\n",
    "            \n",
    "            else:\n",
    "                #pass\n",
    "                if gram[:2] in pos_bi_gram_map:\n",
    "                    prob += math.log( 1.0 / (pos_bi_gram_map[gram[:2]] + len(pos_bi_gram_map)))\n",
    "                else:\n",
    "                    prob += math.log(1.0 / tri_VplusN)\n",
    "                             \n",
    "    probability = math.exp(prob) / len(parsed_tweet) # normalize by the number of grams in the tweet.\n",
    "    return probability\n",
    "   \n",
    "\n",
    "test_tweet = ['Gas', 'by', 'my', 'house', 'hit', '$3.39', '!', '!', '!', '!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', ':)']\n",
    "\n",
    "print positive_probability_calculator(test_tweet,1)\n",
    "print positive_probability_calculator(test_tweet,2)\n",
    "print positive_probability_calculator(test_tweet,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "now we want to make features using the probability calculator!! time to finally get positive probability features for all our tweets.  both training and testing need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text       POS-uni  \\\n",
      "11860  [You, never, know, @realDonaldTrump, frontrunn...  3.737323e-25   \n",
      "1865   [My, takeaway, Mike, Huckabee's, going, privat...  7.709332e-38   \n",
      "1354   [Here's, Donald, Trump, channelling, inner, c*...  2.954993e-23   \n",
      "3641   [My, two, cents:, What,, anything,, learn, las...  1.163266e-32   \n",
      "3421   [#FF, Best, source, 's, going, US, Senate:, @S...  1.789417e-50   \n",
      "\n",
      "             POS-bi       POS-tri  \n",
      "11860  1.443676e-26  1.610607e-24  \n",
      "1865   1.015776e-37  5.441312e-37  \n",
      "1354   1.104814e-20  8.578941e-19  \n",
      "3641   1.038801e-32  4.649673e-32  \n",
      "3421   3.484650e-53  7.041520e-49  \n"
     ]
    }
   ],
   "source": [
    "X_trainy = pd.DataFrame(X_train)  #have to convert the Series into a dataframe, in order to add columns\n",
    "X_trainy['POS-uni'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x, 1),1)\n",
    "X_trainy['POS-bi'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x,2),1)\n",
    "X_trainy['POS-tri'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x,3),1)\n",
    "print X_trainy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text       POS-uni  \\\n",
      "13099  [Jeb, Bush, reminds, elevator, music, ., You, ...  1.148073e-36   \n",
      "10099             [#GOPDebates, Dr, Ben, damn, funny, !]  6.737671e-17   \n",
      "921    [@BernieSanders, @ScottWalker, man, say, one, ...  4.291353e-43   \n",
      "2514   [@EPAespanol, Who's, real, illegal, alien, #GO...  1.894714e-43   \n",
      "11569  [@RealBenCarson, others, getting, time, debate...  1.780546e-42   \n",
      "\n",
      "             POS-bi       POS-tri  \n",
      "13099  7.646237e-41  1.750629e-42  \n",
      "10099  1.979284e-16  2.535393e-14  \n",
      "921    1.396483e-50  5.902548e-54  \n",
      "2514   2.973624e-45  1.781777e-41  \n",
      "11569  8.187831e-52  7.026553e-49  \n"
     ]
    }
   ],
   "source": [
    "X_testy = pd.DataFrame(X_test) #have to convert the Series into a dataframe, in order to add columns\n",
    "X_testy['POS-uni'] = X_testy.text.apply(lambda x: positive_probability_calculator(x, 1), 1)\n",
    "X_testy['POS-bi'] = X_testy.text.apply(lambda x: positive_probability_calculator(x,2),1)\n",
    "X_testy['POS-tri'] = X_testy.text.apply(lambda x: positive_probability_calculator(x,3),1)\n",
    "print X_testy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we can now make a classifier with these features.  This classifier will predict positive labels.  Let's try a few classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            POS-uni        POS-bi       POS-tri\n",
      "11860  3.737323e-25  1.443676e-26  1.610607e-24\n",
      "1865   7.709332e-38  1.015776e-37  5.441312e-37\n",
      "1354   2.954993e-23  1.104814e-20  8.578941e-19\n",
      "3641   1.163266e-32  1.038801e-32  4.649673e-32\n",
      "3421   1.789417e-50  3.484650e-53  7.041520e-49\n",
      "            POS-uni        POS-bi       POS-tri\n",
      "13099  1.148073e-36  7.646237e-41  1.750629e-42\n",
      "10099  6.737671e-17  1.979284e-16  2.535393e-14\n",
      "921    4.291353e-43  1.396483e-50  5.902548e-54\n",
      "2514   1.894714e-43  2.973624e-45  1.781777e-41\n",
      "11569  1.780546e-42  8.187831e-52  7.026553e-49\n"
     ]
    }
   ],
   "source": [
    "#First let's drop the text tweets, they aren't helpful in actual classification\n",
    "X_trainy = X_trainy.drop(X_trainy.columns[0], axis =1)\n",
    "print X_trainy.head()\n",
    "X_testy = X_testy.drop(X_testy.columns[0], axis =1)\n",
    "print X_testy.head()\n",
    "\n",
    "#should be no reason to scale data, because we've normalized it all, it's all probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.0360255665311\n",
      "\n",
      "training confusion:\n",
      "[[8702   15]\n",
      " [1644   31]]\n",
      "\n",
      "testing F1: 0.00704225352113\n",
      "\n",
      "confusion for testing\n",
      "[[2899    7]\n",
      " [ 557    2]]\n",
      "(3465, 3)\n",
      "(10392, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "def basic(clf):\n",
    "    clf.fit(X_trainy, y_train)\n",
    "\n",
    "    x_pred = clf.predict(X_trainy)\n",
    "    F1_train = f1_score(y_train, x_pred)\n",
    "    train_conf = confusion_matrix(y_train, x_pred)\n",
    "    \n",
    "    print \"training F1:\", F1_train\n",
    "    print\n",
    "    print \"training confusion:\\n\", train_conf\n",
    "    print\n",
    "    \n",
    "    y_pred = clf.predict(X_testy)\n",
    "    F1_score = f1_score(y_test, y_pred)\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print \"testing F1:\", F1_score\n",
    "    print\n",
    "    print \"confusion for testing\\n\", conf\n",
    "    \n",
    "basic(clf)\n",
    "print X_testy.shape\n",
    "print X_trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.0\n",
      "\n",
      "training confusion:\n",
      "[[8717    0]\n",
      " [1675    0]]\n",
      "\n",
      "testing F1: 0.0\n",
      "\n",
      "confusion for testing\n",
      "[[2906    0]\n",
      " [ 559    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.0\n",
      "\n",
      "training confusion:\n",
      "[[8717    0]\n",
      " [1675    0]]\n",
      "\n",
      "testing F1: 0.0\n",
      "\n",
      "confusion for testing\n",
      "[[2906    0]\n",
      " [ 559    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.27963679166\n",
      "\n",
      "training confusion:\n",
      "[[ 161 8556]\n",
      " [  12 1663]]\n",
      "\n",
      "testing F1: 0.279210925645\n",
      "\n",
      "confusion for testing\n",
      "[[  63 2843]\n",
      " [   7  552]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "basic(gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.0337405468296\n",
      "\n",
      "training confusion:\n",
      "[[8702   15]\n",
      " [1646   29]]\n",
      "\n",
      "testing F1: 0.0070796460177\n",
      "\n",
      "confusion for testing\n",
      "[[2902    4]\n",
      " [ 557    2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
