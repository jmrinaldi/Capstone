{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 9665 rows, 4 columns\n",
      "                   id   tweet-id             sentiment  \\\n",
      "0  264183816548130816   15140428              positive   \n",
      "1  263405084770172928  591166521              negative   \n",
      "2  262163168678248449   35266263              negative   \n",
      "3  264249301910310912   18516728              negative   \n",
      "4  262682041215234048  254373818  objective-OR-neutral   \n",
      "\n",
      "                                                text  \n",
      "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
      "1                                      Not Available  \n",
      "2                                      Not Available  \n",
      "3  Iranian general says Israel's Iron Dome can't ...  \n",
      "4                                      Not Available  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"downloaded2.tsv\", sep = '\\t')\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "print data.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              sentiment                                               text\n",
      "0              positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1              negative                                      Not Available\n",
      "2              negative                                      Not Available\n",
      "3              negative  Iranian general says Israel's Iron Dome can't ...\n",
      "4  objective-OR-neutral                                      Not Available\n"
     ]
    }
   ],
   "source": [
    "#Let's drop the id-columns, they were used to download the twitter data, with twitter API.\n",
    "df = data.drop(data.columns[[0,1]], axis=1)\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                               text\n",
      "0  positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1  negative  Iranian general says Israel's Iron Dome can't ...\n",
      "2  positive  with J Davlar 11th. Main rivals are team Polan...\n",
      "3  negative  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4  negative  They may have a SuperBowl in Dallas, but Dalla...\n",
      "Dataset has 7549 rows, 2 columns\n",
      "               sentiment                                               text\n",
      "0               positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1               negative  Iranian general says Israel's Iron Dome can't ...\n",
      "2               positive  with J Davlar 11th. Main rivals are team Polan...\n",
      "3               negative  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4               negative  They may have a SuperBowl in Dallas, but Dalla...\n",
      "5                neutral  Im bringing the monster load of candy tomorrow...\n",
      "6   objective-OR-neutral  Apple software, retail chiefs out in overhaul:...\n",
      "7               positive  @oluoch @victor_otti @kunjand I just watched i...\n",
      "8              objective  #Livewire Nadal confirmed for Mexican Open in ...\n",
      "9               positive  @MsSheLahY I didnt want to just pop up... but ...\n",
      "10               neutral  @Alyoup005 @addicted2haley hmmmm  November is ...\n",
      "11             objective  #Iran US delisting MKO from global terrorists ...\n",
      "12              positive  Good Morning Becky ! Thursday is going to be F...\n",
      "13             objective  Expect light-moderate rains over E. Visayas; C...\n",
      "14              positive  One ticket left for the @49ers game tomorrow! ...\n",
      "15              negative  AFC away fans on Saturday. All this stuff abou...\n",
      "16              negative  Why is it so hard to find the @TVGuideMagazine...\n",
      "17  objective-OR-neutral  Game 1 of the NLCS and a rematch of the NFC Ch...\n",
      "18               neutral  @TrevorJavier the heat game may cost alot more...\n",
      "19              positive  Never start working on your dreams and goals t...\n"
     ]
    }
   ],
   "source": [
    "#Now let's drop all the rows in which the tweet was no longer available.\n",
    "df = df[df.text != \"Not Available\"]\n",
    "df = df.reset_index(drop=True) #reset the index after dropping the above rows\n",
    "print df.head()\n",
    "print \"Dataset has {} rows, {} columns\".format(*df.shape)\n",
    "print df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentiment                                               text\n",
      "0           1  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1          -1  Iranian general says Israel's Iron Dome can't ...\n",
      "2           1  with J Davlar 11th. Main rivals are team Polan...\n",
      "3          -1  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4          -1  They may have a SuperBowl in Dallas, but Dalla...\n",
      "5           0  Im bringing the monster load of candy tomorrow...\n",
      "6           0  Apple software, retail chiefs out in overhaul:...\n",
      "7           1  @oluoch @victor_otti @kunjand I just watched i...\n",
      "8           0  #Livewire Nadal confirmed for Mexican Open in ...\n",
      "9           1  @MsSheLahY I didnt want to just pop up... but ...\n",
      "10          0  @Alyoup005 @addicted2haley hmmmm  November is ...\n",
      "11          0  #Iran US delisting MKO from global terrorists ...\n",
      "12          1  Good Morning Becky ! Thursday is going to be F...\n",
      "13          0  Expect light-moderate rains over E. Visayas; C...\n",
      "14          1  One ticket left for the @49ers game tomorrow! ...\n",
      "15         -1  AFC away fans on Saturday. All this stuff abou...\n",
      "16         -1  Why is it so hard to find the @TVGuideMagazine...\n",
      "17          0  Game 1 of the NLCS and a rematch of the NFC Ch...\n",
      "18          0  @TrevorJavier the heat game may cost alot more...\n",
      "19          1  Never start working on your dreams and goals t...\n"
     ]
    }
   ],
   "source": [
    "#Next we need to re-write the neutral / objective labels to all be neutral.  The organizers kept this distinction\n",
    "#for other tasks, but for this task, it's considered the same.\n",
    "# so, let's re-write all objective ->neutral, and all neutral-OR-objective --> neutral.\n",
    "\n",
    "#Since we probably will want our labels numeric (some classifiers may not like 3-way text labels),\n",
    "#we can do that all now.\n",
    "\n",
    "df = df.apply(lambda x: x.replace(['positive', 'negative', 'neutral', 'objective', 'objective-OR-neutral']\n",
    "                                  , [1, -1,0,0,0]) ,1)\n",
    "print df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples is : 7549\n",
      "There are 2820 positive tweets or 0.373559411843%\n",
      "There are 1066 Negative tweets or 0.141210756392%\n",
      "There are 3663 Neutral tweets or 0.485229831766%\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at our class distribution\n",
    "total_tweets = len(df)\n",
    "positive_tweets = sum(df.sentiment == 1)\n",
    "negative_tweets = sum(df.sentiment == -1)\n",
    "neutral_tweets = sum(df.sentiment == 0)\n",
    "\n",
    "print \"The total number of samples is : {}\".format(len(df.sentiment))\n",
    "print \"There are {} positive tweets or {}%\".format \\\n",
    "(positive_tweets, positive_tweets/float(total_tweets) )\n",
    "print \"There are {} Negative tweets or {}%\".format \\\n",
    "(negative_tweets, negative_tweets / float(total_tweets))\n",
    "print \"There are {} Neutral tweets or {}%\".format \\\n",
    "(neutral_tweets, neutral_tweets/ float(total_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$3.39!!!!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat.', ':)']\n"
     ]
    }
   ],
   "source": [
    "# Let's load the texts into lists and remove RT's and URls\n",
    "# we will build a custom function for an individual tweet, \n",
    "#and then use Pandas Dataframe.apply() to run it on all tweets.\n",
    "\n",
    "first_tweet = \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "def parse_tweet (text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "    \n",
    "parsed_tweet = parse_tweet(first_tweet)\n",
    "print parsed_tweet\n",
    "#this results in the most basic splitting operation.  However it gets us very close to what we want.\n",
    "#In the below output the only concern I have is with \"!!!!\" attached to \"$3.39\".  This is not really ideal.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$3.39', '!', '!', '!', '!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', ':)']\n",
      "['Cool', '#cdnpoli', 'Call', 'the', 'hospital', 'in', 'Iqaluit', '&amp;', 'press', '2', 'for', 'English', '.', 'Experience', 'an', 'aboriginal', 'language', 'as', '1st', 'choice']\n",
      "['For', 'how', 'long,', 'i', 'might', 'be', 'in', 'NJ', 'then', '?', '@blove402', 'Thursday', 'Night', 'the', '13th', 'of', 'Dec', '.']\n",
      "['Get', 'ready', 'for', 'our', 'Wednesday', 'Drink', 'Specials', 'Wednesday', '-', '3-8pm', 'Have', 'it', 'your', 'Way', 'Margarita', 'Day', '(', 'Bar', 'Brand', 'Only)', 'DOTDOTDOT']\n"
     ]
    }
   ],
   "source": [
    "#Let's enhance the parser to deal with a few more special cases\n",
    "\n",
    "#compile regex outside of the function, because we will be running this function in a loop.\n",
    "retweets = re.compile(r'(RT ?@.*:)')   \n",
    "urls = re.compile(r'(http:.*\\b)')\n",
    "dotdotdot = re.compile(r'(\\.\\.\\.)')\n",
    "pound_question = re.compile(r'([!\\?])')\n",
    "period_dot = re.compile(r'(\\.(?!\\d))')\n",
    "\n",
    "regex_args = (retweets, urls, dotdotdot, pound_question, period_dot)\n",
    "\n",
    "\n",
    "def parse_tweet (text , retweets, urls, dotdotdot, pound_question, period_dot):\n",
    "    text = re.sub(retweets, \"\", text) #removes RT@thisguy: or RT @thisguy:   two common Retweet bits I dont' need\n",
    "    text = re.sub(urls, \"\", text) # removes URL's\n",
    "    text = re.sub(dotdotdot, ' DOTDOTDOT ', text) #replace '...' with \"DOTDOTDOT' so i preserve the meaning in that token\n",
    "    text = re.sub(pound_question, r' \\1 ', text)  #eyes bleeding? Searches for ! ? and adds white space around them.\n",
    "    text = re.sub(period_dot, r' \\1 ', text) #more blood.  searched for '.' but looks ahead for digits. will not break 3.39\n",
    "  \n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "\n",
    "############\n",
    "#Test cases#\n",
    "############\n",
    "first_tweet = \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "RT_tweet_1 = \"Cool #cdnpoli RT@angelpike: Call the hospital in Iqaluit &amp; press 2 for English. \\\n",
    "Experience an aboriginal language as 1st choice\"\n",
    "RT_tweet_2 = \"For how long, i might be in NJ then?RT @FoolishInApril: @blove402 Thursday Night the 13th of Dec.\"\n",
    "URL_tweet = \"Get ready for our Wednesday Drink Specials Wednesday - 3-8pm Have it your Way Margarita Day \\\n",
    "( Bar Brand Only)... http://t.co/ml806WRT\"\n",
    "\n",
    "test1 = parse_tweet(first_tweet, *regex_args)\n",
    "test2 = parse_tweet(RT_tweet_1, *regex_args)\n",
    "test3 = parse_tweet(RT_tweet_2, *regex_args)\n",
    "test4 = parse_tweet(URL_tweet, *regex_args)\n",
    "print test1\n",
    "print test2\n",
    "print test3\n",
    "print test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                               text\n",
      "0          1  [Gas, by, my, house, hit, $3.39, !, !, !, !, I...\n",
      "1         -1  [Iranian, general, says, Israel's, Iron, Dome,...\n",
      "2          1  [with, J, Davlar, 11th, ., Main, rivals, are, ...\n",
      "3         -1  [Talking, about, ACT's, &amp;&amp;, SAT's,, de...\n",
      "4         -1  [They, may, have, a, SuperBowl, in, Dallas,, b...\n"
     ]
    }
   ],
   "source": [
    "# ok, now that we have rough parsing, lets parse them all!\n",
    "\n",
    "df.text = df.text.apply(lambda x: parse_tweet(x,*regex_args))\n",
    "print df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training tweets:  5661\n",
      "size of testing tweets:  1888\n"
     ]
    }
   ],
   "source": [
    "# now that we have all the tweets parsed, we actually want to split into our training / testing sets. \n",
    "# This is because n-gram analysis (which comes next), should not be done on the testing data!  \n",
    "# The n-gram analysis should on be on training data.  \n",
    "\n",
    "# TODO try to implement n-gram analysis with cross validation, for now I'll use a hold-out testing set\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#Let's split up the labels from the training data\n",
    "\n",
    "X_all = df['text']\n",
    "y_all = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "        X_all, y_all, test_size=0.25, stratify = y_all)\n",
    "\n",
    "print \"size of training tweets: \", len(X_train)\n",
    "print \"size of testing tweets: \", len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  sentiment\n",
      "165   [@kdbowe, But, how, you, gon, handle, it, if, ...          0\n",
      "932   [To, all, my, friends, in, #Turkey, may, you, ...          1\n",
      "4917  [@ColbertReport, RT, Why, does, Steve, Jobs, h...         -1\n",
      "6389  [@TheShannonBurke, after, @immandaschmidt's, c...          0\n",
      "693   [Too, Soon, ?, Amazon, Opens, Black, Friday, D...         -1\n",
      "165     [@kdbowe, But, how, you, gon, handle, it, if, ...\n",
      "932     [To, all, my, friends, in, #Turkey, may, you, ...\n",
      "4917    [@ColbertReport, RT, Why, does, Steve, Jobs, h...\n",
      "6389    [@TheShannonBurke, after, @immandaschmidt's, c...\n",
      "693     [Too, Soon, ?, Amazon, Opens, Black, Friday, D...\n",
      "6444    [Notions, that, the, gap, between, the, rich, ...\n",
      "2277    [@PhuckMeROBYN, you, know, Rih, is, having, a,...\n",
      "5701    [DeSean, Jackson, suppose, to, be, at, Trainin...\n",
      "383     [@sophietump, results, live, at, 10pm, on, Cha...\n",
      "7455    [Watch, Monday, Night, Football, tonight, (&, ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# tricked you! we need to merge the labels and parsed tweets for doing our n-gram analysis.\n",
    "# This is because we will build n-gram models for each class, therefore we need to select only\n",
    "# those tweets that are positive / negative for the two n-gram tables.\n",
    "\n",
    "# Let's re-merge the labels into the training data order to do n-gram analysis\n",
    "XyN_gram = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "print XyN_gram.head()\n",
    "print XyN_gram.text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('.',): 1, (\"I'm\",): 1, ('Hill',): 1, ('!',): 4, ('on',): 1, ('my',): 1, ('$3.39',): 1, ('by',): 1, ('Sat',): 1, ('house',): 1, ('hit',): 1, ('Chapel',): 1, ('to',): 1, ('Gas',): 1, ('going',): 1}\n",
      " \n",
      "{('by', 'my'): 1, ('!', '!'): 3, ('!', \"I'm\"): 1, ('Hill', 'on'): 1, ('going', 'to'): 1, ('my', 'house'): 1, ('Sat', '.'): 1, ('to', 'Chapel'): 1, ('$3.39', '!'): 1, ('house', 'hit'): 1, ('Gas', 'by'): 1, ('hit', '$3.39'): 1, ('Chapel', 'Hill'): 1, (\"I'm\", 'going'): 1, ('on', 'Sat'): 1}\n",
      " \n",
      "{('my', 'house', 'hit'): 1, (\"I'm\", 'going', 'to'): 1, ('Gas', 'by', 'my'): 1, ('on', 'Sat', '.'): 1, ('!', '!', \"I'm\"): 1, ('house', 'hit', '$3.39'): 1, ('!', \"I'm\", 'going'): 1, ('going', 'to', 'Chapel'): 1, ('!', '!', '!'): 2, ('Chapel', 'Hill', 'on'): 1, ('hit', '$3.39', '!'): 1, ('Hill', 'on', 'Sat'): 1, ('to', 'Chapel', 'Hill'): 1, ('$3.39', '!', '!'): 1, ('by', 'my', 'house'): 1}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Let's start by developing a function that will take a parsed tweet and output grams of any size\n",
    "\n",
    "uni_gram_map = {}\n",
    "bi_gram_map = {}\n",
    "tri_gram_map = {}\n",
    "\n",
    "def nGram_counter (parsed_tweet, distance_to_cover, gram_map):\n",
    "    for_loop_range = range(len(parsed_tweet) - distance_to_cover)    \n",
    "    for i in for_loop_range:\n",
    "        gram = tuple(parsed_tweet[i:i+distance_to_cover])\n",
    "        if gram in gram_map:\n",
    "            gram_map[gram] += 1\n",
    "        else:\n",
    "            gram_map[gram] = 1\n",
    "\n",
    "\n",
    "            \n",
    "nGram_counter(test1, 3, tri_gram_map)\n",
    "nGram_counter(test1, 2, bi_gram_map)\n",
    "nGram_counter(test1, 1, uni_gram_map)\n",
    "\n",
    "#Our output should be a dictionaries of all possible tri-grams, bi-grams and unigrams of the tweet\n",
    "#\"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "\n",
    "# if a particular gram exists more than once in a tweet, the counter should have incremented.  We see an example of this\n",
    "# with the token \"!!!!\" which is parsed into \"!!\" 3 times. and \"!!!\" twice. (it overlaps).\n",
    "\n",
    "\n",
    "# check our output.\n",
    "print uni_gram_map\n",
    "print \" \"\n",
    "print bi_gram_map\n",
    "print \" \"\n",
    "print tri_gram_map\n",
    "print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unigrams for Positive Tweets : 10242\n",
      "\n",
      "Total Bi-grams for Positive Tweets: 29328\n",
      "\n",
      "Total Tri-grams for Positive Tweets: 36160\n",
      "\n",
      "Most popular Positive Uni-grams : [(('the',), 1548), (('.',), 1488), (('!',), 1450), (('to',), 995), (('in',), 645), (('on',), 580), (('and',), 552), (('a',), 534), (('I',), 506), (('for',), 493), (('of',), 462), (('is',), 392), (('at',), 391), (('you',), 379), (('be',), 352), (('tomorrow',), 342), (('with',), 336), (('it',), 295), (('my',), 242), (('DOTDOTDOT',), 219), (('?',), 180), (('have',), 176), (('that',), 166), (('night',), 150), (('will',), 149), (('see',), 148), (('this',), 142), (('day',), 141), (('may',), 140), (('me',), 140)]\n",
      "\n",
      "Most Popular Positive  Bi-gams : [(('!', '!'), 422), (('in', 'the'), 150), (('for', 'the'), 126), (('of', 'the'), 120), (('at', 'the'), 114), (('on', 'the'), 108), (('to', 'see'), 92), (('to', 'the'), 90), (('going', 'to'), 85), (('.', '.'), 80), (('.', 'I'), 76), (('will', 'be'), 64), (('to', 'be'), 62), (('is', 'the'), 50), (('the', 'best'), 44), (('tomorrow', '.'), 44), (('tomorrow', '!'), 41), (('tomorrow', 'night'), 38), ((\"Can't\", 'wait'), 38), (('the', '1st'), 38), (('see', 'you'), 37), (('wait', 'to'), 35), (('may', 'be'), 33), (('have', 'a'), 33), (('for', 'a'), 32), (('I', 'love'), 32), (('a', 'good'), 32), (('DOTDOTDOT', '.'), 32), (('!', 'I'), 31), (('forward', 'to'), 30)]\n",
      "\n",
      "Most popular Positive Tri-grams : [(('!', '!', '!'), 171), (('going', 'to', 'be'), 26), (('Looking', 'forward', 'to'), 23), (('to', 'see', 'you'), 22), (('is', 'going', 'to'), 19), (('wait', 'to', 'see'), 19), ((\"Can't\", 'wait', 'to'), 17), (('one', 'of', 'the'), 16), (('be', 'at', 'the'), 14), (('to', 'see', 'the'), 13), ((\"I'm\", 'going', 'to'), 11), ((\"Can't\", 'wait', 'for'), 10), (('going', 'to', 'the'), 10), ((\"can't\", 'wait', 'to'), 10), (('tomorrow', 'night', '!'), 9), (('!', '!', 'I'), 9), (('!', \"Can't\", 'wait'), 8), (('!', 'See', 'you'), 8), (('may', 'be', 'the'), 8), (('are', 'going', 'to'), 8), (('.', \"Can't\", 'wait'), 8), (('Good', 'luck', 'to'), 8), (('.', 'It', 'was'), 7), (('to', 'be', 'a'), 7), (('be', 'the', 'best'), 7), (('night', 'at', 'the'), 7), (('tomorrow', '!', '!'), 7), (('would', 'love', 'to'), 7), (('in', 'the', '1st'), 7), (('the', '2nd', 'time'), 7)]\n"
     ]
    }
   ],
   "source": [
    "#let's now apply our n-gram counter to all the tweets of a certain class.\n",
    "# Let's make the positive n_gram map, on the training data.\n",
    "\n",
    "#for now I will merge all grams into a single map, maybe harder for stats later, but easier for coding now\n",
    "\n",
    "#setup n-gram maps.\n",
    "pos_uni_gram_map ={}\n",
    "pos_bi_gram_map = {}\n",
    "pos_tri_gram_map = {}\n",
    "\n",
    "pos_tweets = XyN_gram[XyN_gram.sentiment == 1]\n",
    "\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 1, pos_uni_gram_map), 1)\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 2, pos_bi_gram_map), 1)\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 3, pos_tri_gram_map), 1)\n",
    "print \"Total Unigrams for Positive Tweets : {}\".format(len(pos_uni_gram_map))\n",
    "print \n",
    "print \"Total Bi-grams for Positive Tweets: {}\".format(len(pos_bi_gram_map))\n",
    "print\n",
    "print \"Total Tri-grams for Positive Tweets: {}\".format(len(pos_tri_gram_map))\n",
    "print\n",
    "print \"Most popular Positive Uni-grams : {}\" \\\n",
    ".format(sorted(pos_uni_gram_map.items(), key=lambda x: x[1], reverse = True)[:30])\n",
    "print\n",
    "print \"Most Popular Positive  Bi-gams : {}\" \\\n",
    ".format(sorted(pos_bi_gram_map.items(), key = lambda x: x[1], reverse = True)[:30])\n",
    "print\n",
    "print \"Most popular Positive Tri-grams : {}\" \\\n",
    ".format(sorted(pos_tri_gram_map.items(), key=lambda x: x[1], reverse = True)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ok, lets use these gram maps to create some features finally.\n",
    "so what we want to do is :\n",
    "\n",
    "take each tweet.text and calculate the probability of that tweet existing as a positive tweet.  \n",
    "we can use this feature to construct our first classifier, for positive tweets.\n",
    "let's start by defining a function that calculates the probability of a tweet.  \n",
    "I will need to include smoothing, normalization and worry about over / underflow.\n",
    "actually the very first step is to transform our maps into maximum likliehood probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "[4.5798030684680556e-05, 4.621072088724584e-05, 4.621072088724584e-05, 4.5798030684680556e-05, 4.62064504204787e-05, 4.6035216940959834e-05, 4.581061890146136e-05, 4.6099944680066385e-05, 0.0002512275893570858, 4.621072088724584e-05]\n"
     ]
    }
   ],
   "source": [
    "# maximum likliehood probabilities for positive grams.\n",
    "# we will calculate maximum likliehood with smoothing, will use simple k-smoothing, with k = 1\n",
    "\n",
    "\n",
    "def calculate_maximum_likliehood (gram_map, k_smoothing = 1, Prior_map = None):\n",
    "    MLE_estimates = {}\n",
    "    total_unique_grams = len(gram_map) # this is V for smoothing \n",
    "    total_gram_count = sum(gram_map.values())\n",
    "    \n",
    "    if Prior_map != None:\n",
    "        total_prior_gram_count = sum(Prior_map.values()) # also V for smoothing on conditioned grams\n",
    "    \n",
    "    #figure out what kind of gram-map we have\n",
    "    keys = gram_map.keys()\n",
    "    if len(keys[0]) == 1: # we have unigrams\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key]+ k_smoothing) / \\\n",
    "            float(total_unique_grams + k_smoothing * total_gram_count)\n",
    "            # above will give MLE with smoothing = 1\n",
    "                \n",
    "    elif len(keys[0]) == 2: # This means we want to condition on previous uni gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float(Prior_map[key[0],] + k_smoothing * total_prior_gram_count)\n",
    "    else: #should be 3 size, so condition on previous bi-gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float(Prior_map[key[:2]] + k_smoothing * total_prior_gram_count)\n",
    "            \n",
    "\n",
    "    return MLE_estimates\n",
    "\n",
    "MLE_pos_uni_gram = calculate_maximum_likliehood(pos_uni_gram_map , 1.0)\n",
    "MLE_pos_bi_gram = calculate_maximum_likliehood(pos_bi_gram_map, 1.0, Prior_map=pos_uni_gram_map)\n",
    "MLE_pos_tri_gram = calculate_maximum_likliehood(pos_tri_gram_map, 1.0, Prior_map=pos_bi_gram_map)\n",
    "\n",
    "## sanity checks\n",
    "print len(MLE_pos_uni_gram) == len(pos_uni_gram_map)\n",
    "print len(MLE_pos_bi_gram) == len(pos_bi_gram_map)\n",
    "print len(MLE_pos_tri_gram) == len(pos_tri_gram_map)\n",
    "\n",
    "# should look reasonble?\n",
    "print MLE_pos_bi_gram.values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ok, now we have MLE for all the training data.  This is n-gram analysis on the positive data.\n",
    "NExt we need to reparse all the tweets, looking up their values in the MLE_pos gram maps.\n",
    "Take log probabilities of everything If a gram doesn't exist in the correct place, then we'll use smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.38314357649e-52\n",
      "1.13047125488e-63\n",
      "2.90426262234e-65\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def v_plus_n(grams):\n",
    "    total_unique_grams = len(grams) # this is V for smoothing \n",
    "    total_gram_count = sum(grams.values()) #this is N\n",
    "    return float(total_unique_grams + total_gram_count)\n",
    "\n",
    "\n",
    "def positive_probability_calculator (parsed_tweet, gram_size):\n",
    "    \n",
    "    # access the gram maps we've calculated before\n",
    "    global MLE_pos_uni_gram\n",
    "    global MLE_pos_bi_gram\n",
    "    global MLE_pos_tri_gram\n",
    "    \n",
    "    global pos_uni_gram_map\n",
    "    global pos_bi_gram_map\n",
    "    \n",
    "    uni_VplusN = v_plus_n(pos_uni_gram_map) # will use these values in smoothing\n",
    "    bi_VplusN = v_plus_n(pos_bi_gram_map)\n",
    "    tri_VplusN = v_plus_n(pos_tri_gram_map)\n",
    "        \n",
    "    # gram_map should correspond to gram_size i.E bi-grams, or tri-grams etc.\n",
    "    loop_range = range(len(parsed_tweet) - gram_size)\n",
    "    prob = 0\n",
    "    \n",
    "    if gram_size == 1: #unigrams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_uni_gram: #look up the probability value we've already calculated\n",
    "                prob += math.log(MLE_pos_uni_gram[gram])\n",
    "            else:  #it's unseen so create a new probability with k-smoothing\n",
    "                prob += math.log( 1.0 / uni_VplusN )  \n",
    "    \n",
    "    if gram_size == 2: #bi-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_bi_gram:\n",
    "                prob += math.log(MLE_pos_bi_gram[gram])  #look up probability we've calculated\n",
    "            \n",
    "            else:  #condition the unseen bi-gram on the seen unigram.\n",
    "                if (gram[0],) in pos_uni_gram_map:\n",
    "                    prob += math.log( 1.0 / (pos_uni_gram_map[gram[0],] + len(pos_uni_gram_map)))  \n",
    "                    \n",
    "                    # so if gram = ('this','cat'), and we have never seen that before.  we are\n",
    "                    # getting a probability that is: 1 / count('this') + count(unique_single grams)\n",
    "                    #obviously close to zero.  ....\n",
    "                else: #then even the first part of this unseen bigram is not the unigram database, just do V+N\n",
    "                    prob += math.log(1.0 / bi_VplusN)\n",
    "    \n",
    "    if gram_size == 3: #tri-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_tri_gram:\n",
    "                prob += math.log(MLE_pos_tri_gram[gram]) # look up prob we've already calculated\n",
    "            \n",
    "            else:\n",
    "                if gram[:2] in pos_bi_gram_map:\n",
    "                    prob += math.log( 1.0 / (pos_bi_gram_map[gram[:2]] + len(pos_bi_gram_map)))\n",
    "                else:\n",
    "                    prob += math.log(1.0 / tri_VplusN)\n",
    "                             \n",
    "    probability = math.exp(prob) / len(parsed_tweet) # normalize by the number of grams in the tweet.\n",
    "    probability = 12000\n",
    "    return probability\n",
    "\n",
    "test_tweet = ['Gas', 'by', 'my', 'house', 'hit', '$3.39', '!', '!', '!', '!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', ':)']\n",
    "\n",
    "print probability_calculator(test_tweet,1)\n",
    "print probability_calculator(test_tweet,2)\n",
    "print probability_calculator(test_tweet,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "now we want to make features using the probability calculator!! time to finally get positive probability features for all our tweets.  both training and testing need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2d8224093e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprob_uni_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobability_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mprob_uni_gram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2167\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2169\u001b[0;31m         \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:62578)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-2d8224093e75>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprob_uni_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobability_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mprob_uni_gram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-b8a27cfe2150>\u001b[0m in \u001b[0;36mprobability_calculator\u001b[0;34m(parsed_tweet, gram_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtri_VplusN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_range\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# normalize by the number of grams in the tweet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "X_all.head()\n",
    "prob_uni_gram = X_all.apply(lambda x: probability_calculator(x, 1), 1)\n",
    "print prob_uni_gram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
