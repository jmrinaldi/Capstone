{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 9665 rows, 4 columns\n",
      "                   id   tweet-id             sentiment  \\\n",
      "0  264183816548130816   15140428              positive   \n",
      "1  263405084770172928  591166521              negative   \n",
      "2  262163168678248449   35266263              negative   \n",
      "3  264249301910310912   18516728              negative   \n",
      "4  262682041215234048  254373818  objective-OR-neutral   \n",
      "\n",
      "                                                text  \n",
      "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
      "1                                      Not Available  \n",
      "2                                      Not Available  \n",
      "3  Iranian general says Israel's Iron Dome can't ...  \n",
      "4                                      Not Available  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"downloaded2.tsv\", sep = '\\t')\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "print data.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              sentiment                                               text\n",
      "0              positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1              negative                                      Not Available\n",
      "2              negative                                      Not Available\n",
      "3              negative  Iranian general says Israel's Iron Dome can't ...\n",
      "4  objective-OR-neutral                                      Not Available\n"
     ]
    }
   ],
   "source": [
    "#Let's drop the id-columns, they were used to download the twitter data, with twitter API.\n",
    "df = data.drop(data.columns[[0,1]], axis=1)\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                               text\n",
      "0  positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1  negative  Iranian general says Israel's Iron Dome can't ...\n",
      "2  positive  with J Davlar 11th. Main rivals are team Polan...\n",
      "3  negative  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4  negative  They may have a SuperBowl in Dallas, but Dalla...\n",
      "Dataset has 7549 rows, 2 columns\n",
      "               sentiment                                               text\n",
      "0               positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1               negative  Iranian general says Israel's Iron Dome can't ...\n",
      "2               positive  with J Davlar 11th. Main rivals are team Polan...\n",
      "3               negative  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4               negative  They may have a SuperBowl in Dallas, but Dalla...\n",
      "5                neutral  Im bringing the monster load of candy tomorrow...\n",
      "6   objective-OR-neutral  Apple software, retail chiefs out in overhaul:...\n",
      "7               positive  @oluoch @victor_otti @kunjand I just watched i...\n",
      "8              objective  #Livewire Nadal confirmed for Mexican Open in ...\n",
      "9               positive  @MsSheLahY I didnt want to just pop up... but ...\n",
      "10               neutral  @Alyoup005 @addicted2haley hmmmm  November is ...\n",
      "11             objective  #Iran US delisting MKO from global terrorists ...\n",
      "12              positive  Good Morning Becky ! Thursday is going to be F...\n",
      "13             objective  Expect light-moderate rains over E. Visayas; C...\n",
      "14              positive  One ticket left for the @49ers game tomorrow! ...\n",
      "15              negative  AFC away fans on Saturday. All this stuff abou...\n",
      "16              negative  Why is it so hard to find the @TVGuideMagazine...\n",
      "17  objective-OR-neutral  Game 1 of the NLCS and a rematch of the NFC Ch...\n",
      "18               neutral  @TrevorJavier the heat game may cost alot more...\n",
      "19              positive  Never start working on your dreams and goals t...\n"
     ]
    }
   ],
   "source": [
    "#Now let's drop all the rows in which the tweet was no longer available.\n",
    "df = df[df.text != \"Not Available\"]\n",
    "df = df.reset_index(drop=True) #reset the index after dropping the above rows\n",
    "print df.head()\n",
    "print \"Dataset has {} rows, {} columns\".format(*df.shape)\n",
    "print df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentiment                                               text\n",
      "0           1  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1           0  Iranian general says Israel's Iron Dome can't ...\n",
      "2           1  with J Davlar 11th. Main rivals are team Polan...\n",
      "3           0  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4           0  They may have a SuperBowl in Dallas, but Dalla...\n",
      "5           0  Im bringing the monster load of candy tomorrow...\n",
      "6           0  Apple software, retail chiefs out in overhaul:...\n",
      "7           1  @oluoch @victor_otti @kunjand I just watched i...\n",
      "8           0  #Livewire Nadal confirmed for Mexican Open in ...\n",
      "9           1  @MsSheLahY I didnt want to just pop up... but ...\n",
      "10          0  @Alyoup005 @addicted2haley hmmmm  November is ...\n",
      "11          0  #Iran US delisting MKO from global terrorists ...\n",
      "12          1  Good Morning Becky ! Thursday is going to be F...\n",
      "13          0  Expect light-moderate rains over E. Visayas; C...\n",
      "14          1  One ticket left for the @49ers game tomorrow! ...\n",
      "15          0  AFC away fans on Saturday. All this stuff abou...\n",
      "16          0  Why is it so hard to find the @TVGuideMagazine...\n",
      "17          0  Game 1 of the NLCS and a rematch of the NFC Ch...\n",
      "18          0  @TrevorJavier the heat game may cost alot more...\n",
      "19          1  Never start working on your dreams and goals t...\n"
     ]
    }
   ],
   "source": [
    "#Next we need to re-write the neutral / objective labels to all be neutral.  The organizers kept this distinction\n",
    "#for other tasks, but for this task, it's considered the same.\n",
    "# so, let's re-write all objective ->neutral, and all neutral-OR-objective --> neutral.\n",
    "\n",
    "#Since we probably will want our labels numeric (some classifiers may not like 3-way text labels),\n",
    "#we can do that all now.\n",
    "\n",
    "df = df.apply(lambda x: x.replace(['positive', 'negative', 'neutral', 'objective', 'objective-OR-neutral']\n",
    "                                  , [1, 0,0,0,0]) ,1)\n",
    "print df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples is : 7549\n",
      "There are 2820 positive tweets or 0.373559411843%\n",
      "There are 0 Negative tweets or 0.0%\n",
      "There are 4729 Neutral tweets or 0.626440588157%\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at our class distribution\n",
    "total_tweets = len(df)\n",
    "positive_tweets = sum(df.sentiment == 1)\n",
    "negative_tweets = sum(df.sentiment == -1)\n",
    "neutral_tweets = sum(df.sentiment == 0)\n",
    "\n",
    "print \"The total number of samples is : {}\".format(len(df.sentiment))\n",
    "print \"There are {} positive tweets or {}%\".format \\\n",
    "(positive_tweets, positive_tweets/float(total_tweets) )\n",
    "print \"There are {} Negative tweets or {}%\".format \\\n",
    "(negative_tweets, negative_tweets / float(total_tweets))\n",
    "print \"There are {} Neutral tweets or {}%\".format \\\n",
    "(neutral_tweets, neutral_tweets/ float(total_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$3.39!!!!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat.', ':)']\n"
     ]
    }
   ],
   "source": [
    "# Let's load the texts into lists and remove RT's and URls\n",
    "# we will build a custom function for an individual tweet, \n",
    "#and then use Pandas Dataframe.apply() to run it on all tweets.\n",
    "\n",
    "first_tweet = \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "def parse_tweet (text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "    \n",
    "parsed_tweet = parse_tweet(first_tweet)\n",
    "print parsed_tweet\n",
    "#this results in the most basic splitting operation.  However it gets us very close to what we want.\n",
    "#In the below output the only concern I have is with \"!!!!\" attached to \"$3.39\".  This is not really ideal.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours\\tourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "#stop-word removal\n",
    "with open('stopwords.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "print stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gas', 'house', 'hit', '$3.39', '!', '!', '!', '!', \"'m\", 'going', 'chapel', 'hill', 'sat', '.', ':)']\n",
      "['cool', '#cdnpoli', 'rt@angelpike:', 'call', 'hospital', 'iqaluit', '&amp;', 'press', '2', 'english', '.', 'experience', 'aboriginal', 'language', '1st', 'choice']\n",
      "['long,', 'might', 'nj', '?', 'rt', '@foolishinapril:', '@blove402', 'thursday', 'night', '13th', 'dec', '.']\n",
      "['get', 'ready', 'wednesday', 'drink', 'specials', 'wednesday', '-', '3-8pm', 'way', 'margarita', 'day', '(', 'bar', 'brand', ')', 'DOTDOTDOT']\n"
     ]
    }
   ],
   "source": [
    "#Let's enhance the parser to deal with a few more special cases\n",
    "\n",
    "#compile regex outside of the function, because we will be running this function in a loop.\n",
    "retweets = re.compile(r'(RT ?@.*:)')   \n",
    "urls = re.compile(r'(http:.*\\b)')\n",
    "dotdotdot = re.compile(r'(\\.\\.\\.)')\n",
    "pound_question = re.compile(r'([!\\?])')\n",
    "period_dot = re.compile(r'(\\.(?!\\d))')\n",
    "stop_word = re.compile(r'\\b(?:%s)\\b' % '|'.join(stop_words))\n",
    "\n",
    "regex_args = (retweets, urls, dotdotdot, pound_question, period_dot, stop_word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_tweet (text , retweets, urls, dotdotdot, pound_question, period_dot, stop_word):\n",
    "    text = text.lower()\n",
    "    text = re.sub(retweets, \"\", text) #removes RT@thisguy: or RT @thisguy:   two common Retweet bits I dont' need\n",
    "    text = re.sub(urls, \"\", text) # removes URL's\n",
    "    text = re.sub(dotdotdot, ' DOTDOTDOT ', text) #replace '...' with \"DOTDOTDOT' so i preserve the meaning in that token\n",
    "    text = re.sub(pound_question, r' \\1 ', text)  #eyes bleeding? Searches for ! ? and adds white space around them.\n",
    "    text = re.sub(period_dot, r' \\1 ', text) #more blood.  searched for '.' but looks ahead for digits. will not break 3.39\n",
    "    text = re.sub(stop_word, \"\", text) #removes stop words.\n",
    "\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "\n",
    "############\n",
    "#Test cases#\n",
    "############\n",
    "first_tweet = \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "RT_tweet_1 = \"Cool #cdnpoli RT@angelpike: Call the hospital in Iqaluit &amp; press 2 for English. \\\n",
    "Experience an aboriginal language as 1st choice\"\n",
    "RT_tweet_2 = \"For how long, i might be in NJ then?RT @FoolishInApril: @blove402 Thursday Night the 13th of Dec.\"\n",
    "URL_tweet = \"Get ready for our Wednesday Drink Specials Wednesday - 3-8pm Have it your Way Margarita Day \\\n",
    "( Bar Brand Only)... http://t.co/ml806WRT\"\n",
    "\n",
    "test1 = parse_tweet(first_tweet, *regex_args)\n",
    "test2 = parse_tweet(RT_tweet_1, *regex_args)\n",
    "test3 = parse_tweet(RT_tweet_2, *regex_args)\n",
    "test4 = parse_tweet(URL_tweet, *regex_args)\n",
    "print test1\n",
    "print test2\n",
    "print test3\n",
    "print test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                               text\n",
      "0          1  [gas, house, hit, $3.39, !, !, !, !, 'm, going...\n",
      "1          0  [iranian, general, says, israel's, iron, dome,...\n",
      "2          1  [j, davlar, 11th, ., main, rivals, team, polan...\n",
      "3          0  [talking, act's, &amp;&amp;, sat's,, deciding,...\n",
      "4          0  [may, superbowl, dallas,, dallas, ain't, winni...\n",
      "(7549, 2)\n"
     ]
    }
   ],
   "source": [
    "# ok, now that we have rough parsing, lets parse them all!\n",
    "df.text = df.text.apply(lambda x: parse_tweet(x,*regex_args))\n",
    "print df.head()\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7536, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop some tweets that got parsed to zero\n",
    "df = df[df['text'].map(len) >= 1]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training tweets:  5652\n",
      "size of testing tweets:  1884\n"
     ]
    }
   ],
   "source": [
    "# now that we have all the tweets parsed, we actually want to split into our training / testing sets. \n",
    "# This is because n-gram analysis (which comes next), should not be done on the testing data!  \n",
    "# The n-gram analysis should on be on training data.  \n",
    "\n",
    "# TODO try to implement n-gram analysis with cross validation, for now I'll use a hold-out testing set\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#Let's split up the labels from the training data\n",
    "\n",
    "X_all = df['text']\n",
    "y_all = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "        X_all, y_all, test_size=0.25, stratify = y_all)\n",
    "\n",
    "print \"size of training tweets: \", len(X_train)\n",
    "print \"size of testing tweets: \", len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  sentiment\n",
      "6497  [@woolimboys, oppa, may, ur, style, like, hoya...          1\n",
      "5059  [summerslam, may, interesting, !, @cmpunk, sav...          1\n",
      "4743  [nicki, miinaj, talks, robin, roberts, good, m...          0\n",
      "5115  [@rinecho9239, good, morning, irene, ^_^, nice...          1\n",
      "1273  [@_schoolboyj, top, 5, sg, 2, 6th, men, ., lef...          0\n",
      "6497    [@woolimboys, oppa, may, ur, style, like, hoya...\n",
      "5059    [summerslam, may, interesting, !, @cmpunk, sav...\n",
      "4743    [nicki, miinaj, talks, robin, roberts, good, m...\n",
      "5115    [@rinecho9239, good, morning, irene, ^_^, nice...\n",
      "1273    [@_schoolboyj, top, 5, sg, 2, 6th, men, ., lef...\n",
      "2464    [rocking, machine, gun, kelly, tomorrow, night...\n",
      "3546    ['really, good, day', danica, patrick, finishe...\n",
      "3492    [listening, ni**, paris, [explicit], jay, z, k...\n",
      "5863    ['s, singapore's, independence, day, today, !,...\n",
      "7262    [just, read, article, DOTDOTDOT, e, l, james, ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# tricked you! we need to merge the labels and parsed tweets for doing our n-gram analysis.\n",
    "# This is because we will build n-gram models for each class, therefore we need to select only\n",
    "# those tweets that are positive / negative for the two n-gram tables.\n",
    "\n",
    "# Let's re-merge the labels into the training data order to do n-gram analysis\n",
    "XyN_gram = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "print XyN_gram.head()\n",
    "print XyN_gram.text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('.',): 1, (\"'m\",): 1, ('hill',): 1, ('!',): 4, ('$3.39',): 1, ('house',): 1, ('hit',): 1, ('chapel',): 1, ('gas',): 1, ('sat',): 1, ('going',): 1}\n",
      " \n",
      "{('going', 'chapel'): 1, ('!', '!'): 3, ('!', \"'m\"): 1, (\"'m\", 'going'): 1, ('sat', '.'): 1, ('$3.39', '!'): 1, ('hill', 'sat'): 1, ('house', 'hit'): 1, ('hit', '$3.39'): 1, ('chapel', 'hill'): 1, ('gas', 'house'): 1}\n",
      " \n",
      "{('chapel', 'hill', 'sat'): 1, ('hill', 'sat', '.'): 1, ('!', '!', \"'m\"): 1, ('house', 'hit', '$3.39'): 1, (\"'m\", 'going', 'chapel'): 1, ('!', \"'m\", 'going'): 1, ('gas', 'house', 'hit'): 1, ('!', '!', '!'): 2, ('hit', '$3.39', '!'): 1, ('going', 'chapel', 'hill'): 1, ('$3.39', '!', '!'): 1}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Let's start by developing a function that will take a parsed tweet and output grams of any size\n",
    "\n",
    "uni_gram_map = {}\n",
    "bi_gram_map = {}\n",
    "tri_gram_map = {}\n",
    "\n",
    "def nGram_counter (parsed_tweet, distance_to_cover, gram_map):\n",
    "    for_loop_range = range(len(parsed_tweet) - distance_to_cover)    \n",
    "    for i in for_loop_range:\n",
    "        gram = tuple(parsed_tweet[i:i+distance_to_cover])\n",
    "        if gram in gram_map:\n",
    "            gram_map[gram] += 1\n",
    "        else:\n",
    "            gram_map[gram] = 1\n",
    "\n",
    "\n",
    "            \n",
    "nGram_counter(test1, 3, tri_gram_map)\n",
    "nGram_counter(test1, 2, bi_gram_map)\n",
    "nGram_counter(test1, 1, uni_gram_map)\n",
    "\n",
    "#Our output should be a dictionaries of all possible tri-grams, bi-grams and unigrams of the tweet\n",
    "#\"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "\n",
    "# if a particular gram exists more than once in a tweet, the counter should have incremented.  We see an example of this\n",
    "# with the token \"!!!!\" which is parsed into \"!!\" 3 times. and \"!!!\" twice. (it overlaps).\n",
    "\n",
    "\n",
    "# check our output.\n",
    "print uni_gram_map\n",
    "print \" \"\n",
    "print bi_gram_map\n",
    "print \" \"\n",
    "print tri_gram_map\n",
    "print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unigrams for Positive Tweets : 8663\n",
      "\n",
      "Total Bi-grams for Positive Tweets: 22600\n",
      "\n",
      "Total Tri-grams for Positive Tweets: 24451\n",
      "\n",
      "Most popular Positive Uni-grams : [(('!',), 1513), (('.',), 1480), (('tomorrow',), 380), (('DOTDOTDOT',), 228), (('day',), 215), (('night',), 204), ((\"'s\",), 203), (('good',), 192), (('?',), 189), (('just',), 174), (('will',), 167), (('see',), 167), (('may',), 166), (('tonight',), 151), (('love',), 146), (('going',), 134), ((\"'m\",), 123), (('friday',), 116), (('time',), 114), ((',',), 114), (('sunday',), 114), (('game',), 111), (('get',), 110), (('great',), 108), (('one',), 106), (('&amp;',), 105), (('like',), 105), (('1st',), 103), (('happy',), 98), (('come',), 98)]\n",
      "\n",
      "Most Popular Positive  Bi-gams : [(('!', '!'), 421), (('.', '.'), 85), (('tomorrow', '.'), 43), (('tomorrow', '!'), 42), (('DOTDOTDOT', '.'), 37), (('tomorrow', 'night'), 35), (('.', \"'s\"), 33), (('looking', 'forward'), 30), (('tonight', '!'), 28), (('.', 'may'), 26), (('wait', 'see'), 24), (('good', 'morning'), 23), ((\"'m\", 'going'), 23), (('tonight', '.'), 23), (('day', '!'), 22), (('night', '!'), 21), (('good', 'luck'), 21), (('night', '.'), 20), (('!', 'hope'), 19), (('friday', 'night'), 19), (('today', '.'), 19), (('last', 'night'), 19), (('!', \"'s\"), 18), (('?', '?'), 17), (('.', \"'m\"), 17), (('saturday', '.'), 17), (('.', \"'ll\"), 16), (('.', 'love'), 16), (('.', 'good'), 16), (('!', \"'m\"), 16)]\n",
      "\n",
      "Most popular Positive Tri-grams : [(('!', '!', '!'), 171), (('tomorrow', 'night', '!'), 8), (('tomorrow', '!', '!'), 7), (('sarah', 'g', '.'), 7), (('!', 'hope', 'see'), 7), (('DOTDOTDOT', '.', '.'), 6), (('!', '!', 'love'), 6), (('tonight', '!', '!'), 6), (('last', 'night', '.'), 6), (('world', 'book', 'day'), 6), (('love', '!', '!'), 6), (('!', 'looking', 'forward'), 5), (('!', \"'m\", 'going'), 5), (('day', 'tomorrow', '!'), 5), (('monday', 'night', 'football'), 5), (('celebrity', 'juice', 'tonight'), 5), (('.', 'looking', 'forward'), 5), (('sm', 'sta', '.'), 5), (('g', '.', 'day'), 5), (('tomorrow', '.', '.'), 5), (('happy', 'valentines', 'day'), 4), (('teen', 'choice', 'awards'), 4), (('!', '!', 'get'), 4), (('board', 'said', 'sunday'), 4), (('hopeful', 'bangladesh', 'visit:'), 4), (('pakistan', 'hopeful', 'bangladesh'), 4), (('houston', 'rodeo', 'march'), 4), (('sarah', 'g', 'live'), 4), (('cricket', 'board', 'said'), 4), (('day', '!', '!'), 4)]\n"
     ]
    }
   ],
   "source": [
    "#let's now apply our n-gram counter to all the tweets of a certain class.\n",
    "# Let's make the positive n_gram map, on the training data.\n",
    "\n",
    "#for now I will merge all grams into a single map, maybe harder for stats later, but easier for coding now\n",
    "\n",
    "#setup n-gram maps.\n",
    "pos_uni_gram_map ={}\n",
    "pos_bi_gram_map = {}\n",
    "pos_tri_gram_map = {}\n",
    "\n",
    "pos_tweets = XyN_gram[XyN_gram.sentiment == 1]\n",
    "\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 1, pos_uni_gram_map), 1)\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 2, pos_bi_gram_map), 1)\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 3, pos_tri_gram_map), 1)\n",
    "print \"Total Unigrams for Positive Tweets : {}\".format(len(pos_uni_gram_map))\n",
    "print \n",
    "print \"Total Bi-grams for Positive Tweets: {}\".format(len(pos_bi_gram_map))\n",
    "print\n",
    "print \"Total Tri-grams for Positive Tweets: {}\".format(len(pos_tri_gram_map))\n",
    "print\n",
    "print \"Most popular Positive Uni-grams : {}\" \\\n",
    ".format(sorted(pos_uni_gram_map.items(), key=lambda x: x[1], reverse = True)[:30])\n",
    "print\n",
    "print \"Most Popular Positive  Bi-gams : {}\" \\\n",
    ".format(sorted(pos_bi_gram_map.items(), key = lambda x: x[1], reverse = True)[:30])\n",
    "print\n",
    "print \"Most popular Positive Tri-grams : {}\" \\\n",
    ".format(sorted(pos_tri_gram_map.items(), key=lambda x: x[1], reverse = True)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ok, lets use these gram maps to create some features finally.\n",
    "so what we want to do is :\n",
    "\n",
    "take each tweet.text and calculate the probability of that tweet existing as a positive tweet.  \n",
    "we can use this feature to construct our first classifier, for positive tweets.\n",
    "let's start by defining a function that calculates the probability of a tweet.  \n",
    "I will need to include smoothing, normalization and worry about over / underflow.\n",
    "actually the very first step is to transform our maps into maximum likliehood probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "[6.720881779689495e-05, 6.717946995398206e-05, 6.719978496068813e-05, 6.720881779689495e-05, 0.00010046885465505693, 6.705334093271198e-05, 6.720430107526882e-05, 9.603995262029004e-05, 6.688292144600876e-05, 6.718849732925722e-05]\n"
     ]
    }
   ],
   "source": [
    "# maximum likliehood probabilities for positive grams.\n",
    "# we will calculate maximum likliehood with smoothing, will use simple k-smoothing, with k = 1\n",
    "\n",
    "\n",
    "def calculate_maximum_likliehood (gram_map, k_smoothing = 1, Prior_map = None):\n",
    "    MLE_estimates = {}\n",
    "    total_unique_grams = len(gram_map) # this is V for smoothing \n",
    "    total_gram_count = sum(gram_map.values())\n",
    "    \n",
    "    if Prior_map != None:\n",
    "        total_prior_gram_count = sum(Prior_map.values()) # also V for smoothing on conditioned grams\n",
    "    \n",
    "    #figure out what kind of gram-map we have\n",
    "    keys = gram_map.keys()\n",
    "    if len(keys[0]) == 1: # we have unigrams\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key]+ k_smoothing) / \\\n",
    "            float(total_unique_grams + k_smoothing * total_gram_count)\n",
    "            # above will give MLE with smoothing = 1\n",
    "                \n",
    "    elif len(keys[0]) == 2: # This means we want to condition on previous uni gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float(Prior_map[key[0],] + k_smoothing * total_prior_gram_count)\n",
    "    else: #should be 3 size, so condition on previous bi-gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float(Prior_map[key[:2]] + k_smoothing * total_prior_gram_count)\n",
    "            \n",
    "\n",
    "    return MLE_estimates\n",
    "\n",
    "MLE_pos_uni_gram = calculate_maximum_likliehood(pos_uni_gram_map , 1)\n",
    "MLE_pos_bi_gram = calculate_maximum_likliehood(pos_bi_gram_map, 1, Prior_map=pos_uni_gram_map)\n",
    "MLE_pos_tri_gram = calculate_maximum_likliehood(pos_tri_gram_map, 1, Prior_map=pos_bi_gram_map)\n",
    "\n",
    "## sanity checks\n",
    "print len(MLE_pos_uni_gram) == len(pos_uni_gram_map)\n",
    "print len(MLE_pos_bi_gram) == len(pos_bi_gram_map)\n",
    "print len(MLE_pos_tri_gram) == len(pos_tri_gram_map)\n",
    "\n",
    "# should look reasonble?\n",
    "print MLE_pos_bi_gram.values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ok, now we have MLE for all the training data.  This is n-gram analysis on the positive data.\n",
    "NExt we need to reparse all the tweets, looking up their values in the MLE_pos gram maps.\n",
    "Take log probabilities of everything If a gram doesn't exist in the correct place, then we'll use smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.40924012898e-64\n",
      "2.14769015248e-70\n",
      "3.34158811291e-70\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def v_plus_n(grams):\n",
    "    total_unique_grams = len(grams) # this is V for smoothing \n",
    "    total_gram_count = sum(grams.values()) #this is N\n",
    "    return float(total_unique_grams + total_gram_count)\n",
    "\n",
    "\n",
    "def positive_probability_calculator (parsed_tweet, gram_size):\n",
    "    \n",
    "    if len(parsed_tweet) <1:  #this will catch any empty tweets I missed earlier.\n",
    "        return \"NaN\"\n",
    "    \n",
    "    # access the gram maps we've calculated before\n",
    "    global MLE_pos_uni_gram\n",
    "    global MLE_pos_bi_gram\n",
    "    global MLE_pos_tri_gram\n",
    "    \n",
    "    global pos_uni_gram_map\n",
    "    global pos_bi_gram_map\n",
    "    \n",
    "    uni_VplusN = v_plus_n(pos_uni_gram_map) # will use these values in smoothing\n",
    "    bi_VplusN = v_plus_n(pos_bi_gram_map)\n",
    "    tri_VplusN = v_plus_n(pos_tri_gram_map)\n",
    "        \n",
    "    # gram_map should correspond to gram_size i.E bi-grams, or tri-grams etc.\n",
    "    loop_range = range(len(parsed_tweet) - gram_size)\n",
    "    prob = 0\n",
    "    \n",
    "    if gram_size == 1: #unigrams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_uni_gram: #look up the probability value we've already calculated\n",
    "                prob += math.log(MLE_pos_uni_gram[gram])\n",
    "            else:  #it's unseen so create a new probability with k-smoothing\n",
    "                #pass # penalize it with nothing\n",
    "                prob += math.log( 1.0 / uni_VplusN )  \n",
    "    \n",
    "    if gram_size == 2: #bi-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_bi_gram:\n",
    "                prob += math.log(MLE_pos_bi_gram[gram])  #look up probability we've calculated\n",
    "            \n",
    "            else:  #condition the unseen bi-gram on the seen unigram.\n",
    "                #pass\n",
    "                if (gram[0],) in pos_uni_gram_map:\n",
    "                    prob += math.log( 1.0 / (pos_uni_gram_map[gram[0],] + len(pos_uni_gram_map)))  \n",
    "                    \n",
    "                    # so if gram = ('this','cat'), and we have never seen that before.  we are\n",
    "                    # getting a probability that is: 1 / count('this') + count(unique_single grams)\n",
    "                    #obviously close to zero.  ....\n",
    "                else: #then even the first part of this unseen bigram is not the unigram database, just do V+N\n",
    "                    prob += math.log(1.0 / bi_VplusN)\n",
    "    \n",
    "    if gram_size == 3: #tri-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_tri_gram:\n",
    "                prob += math.log(MLE_pos_tri_gram[gram]) # look up prob we've already calculated\n",
    "            \n",
    "            else:\n",
    "                #pass\n",
    "                if gram[:2] in pos_bi_gram_map:\n",
    "                    prob += math.log( 1.0 / (pos_bi_gram_map[gram[:2]] + len(pos_bi_gram_map)))\n",
    "                else:\n",
    "                    prob += math.log(1.0 / tri_VplusN)\n",
    "                             \n",
    "    probability = math.exp(prob) / len(parsed_tweet) # normalize by the number of grams in the tweet.\n",
    "    return probability\n",
    "   \n",
    "\n",
    "test_tweet = ['Gas', 'by', 'my', 'house', 'hit', '$3.39', '!', '!', '!', '!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', ':)']\n",
    "\n",
    "print positive_probability_calculator(test_tweet,1)\n",
    "print positive_probability_calculator(test_tweet,2)\n",
    "print positive_probability_calculator(test_tweet,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "now we want to make features using the probability calculator!! time to finally get positive probability features for all our tweets.  both training and testing need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       POS-uni  \\\n",
      "6497  [@woolimboys, oppa, may, ur, style, like, hoya...  4.291341e-58   \n",
      "5059  [summerslam, may, interesting, !, @cmpunk, sav...  4.429611e-25   \n",
      "4743  [nicki, miinaj, talks, robin, roberts, good, m...  1.923701e-53   \n",
      "5115  [@rinecho9239, good, morning, irene, ^_^, nice...  4.194214e-24   \n",
      "1273  [@_schoolboyj, top, 5, sg, 2, 6th, men, ., lef...  1.272716e-36   \n",
      "\n",
      "            POS-bi       POS-tri  \n",
      "6497  1.802216e-68  4.324426e-64  \n",
      "5059  3.878514e-30  1.593081e-26  \n",
      "4743  2.761789e-54  4.839959e-57  \n",
      "5115  2.053266e-25  3.712847e-22  \n",
      "1273  2.800281e-38  5.161599e-39  \n"
     ]
    }
   ],
   "source": [
    "X_trainy = pd.DataFrame(X_train)  #have to convert the Series into a dataframe, in order to add columns\n",
    "X_trainy['POS-uni'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x, 1),1)\n",
    "X_trainy['POS-bi'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x,2),1)\n",
    "X_trainy['POS-tri'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x,3),1)\n",
    "print X_trainy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       POS-uni  \\\n",
      "2641  [birmingham's, zulu's, took, 800, lads, leeds,...  1.506628e-63   \n",
      "52    [alex, poythress, 11, points, 7, rebounds, deb...  1.385190e-56   \n",
      "2083  [may, known, steve, sabol's, name,, chances, k...  1.918849e-45   \n",
      "3441        [@jazzpastord, back, az, ?, see, sunday, ?]  2.885970e-20   \n",
      "1976  [@bigtexasschan, fux, wit, yo, 3rd, choice, gi...  6.340600e-76   \n",
      "\n",
      "            POS-bi       POS-tri  \n",
      "2641  1.566195e-70  1.695331e-76  \n",
      "52    2.703452e-58  1.137296e-62  \n",
      "2083  2.734361e-46  3.866100e-48  \n",
      "3441  4.261310e-22  2.289009e-20  \n",
      "1976  2.725301e-75  7.672101e-77  \n"
     ]
    }
   ],
   "source": [
    "X_testy = pd.DataFrame(X_test) #have to convert the Series into a dataframe, in order to add columns\n",
    "X_testy['POS-uni'] = X_testy.text.apply(lambda x: positive_probability_calculator(x, 1), 1)\n",
    "X_testy['POS-bi'] = X_testy.text.apply(lambda x: positive_probability_calculator(x,2),1)\n",
    "X_testy['POS-tri'] = X_testy.text.apply(lambda x: positive_probability_calculator(x,3),1)\n",
    "print X_testy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we can now make a classifier with these features.  This classifier will predict positive labels.  Let's try a few classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           POS-uni        POS-bi       POS-tri\n",
      "6497  4.291341e-58  1.802216e-68  4.324426e-64\n",
      "5059  4.429611e-25  3.878514e-30  1.593081e-26\n",
      "4743  1.923701e-53  2.761789e-54  4.839959e-57\n",
      "5115  4.194214e-24  2.053266e-25  3.712847e-22\n",
      "1273  1.272716e-36  2.800281e-38  5.161599e-39\n",
      "           POS-uni        POS-bi       POS-tri\n",
      "2641  1.506628e-63  1.566195e-70  1.695331e-76\n",
      "52    1.385190e-56  2.703452e-58  1.137296e-62\n",
      "2083  1.918849e-45  2.734361e-46  3.866100e-48\n",
      "3441  2.885970e-20  4.261310e-22  2.289009e-20\n",
      "1976  6.340600e-76  2.725301e-75  7.672101e-77\n"
     ]
    }
   ],
   "source": [
    "#First let's drop the text tweets, they aren't helpful in actual classification\n",
    "X_trainy = X_trainy.drop(X_trainy.columns[0], axis =1)\n",
    "print X_trainy.head()\n",
    "X_testy = X_testy.drop(X_testy.columns[0], axis =1)\n",
    "print X_testy.head()\n",
    "\n",
    "#should be no reason to scale data, because we've normalized it all, it's all probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.00283419933869\n",
      "\n",
      "training confusion:\n",
      "[[3538    0]\n",
      " [2111    3]]\n",
      "\n",
      "testing F1: 0.002828854314\n",
      "\n",
      "confusion for testing\n",
      "[[1178    1]\n",
      " [ 704    1]]\n",
      "(1884, 3)\n",
      "(5652, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "def basic(clf):\n",
    "    clf.fit(X_trainy, y_train)\n",
    "\n",
    "    x_pred = clf.predict(X_trainy)\n",
    "    F1_train = f1_score(y_train, x_pred)\n",
    "    train_conf = confusion_matrix(y_train, x_pred)\n",
    "    \n",
    "    print \"training F1:\", F1_train\n",
    "    print\n",
    "    print \"training confusion:\\n\", train_conf\n",
    "    print\n",
    "    \n",
    "    y_pred = clf.predict(X_testy)\n",
    "    F1_score = f1_score(y_test, y_pred)\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print \"testing F1:\", F1_score\n",
    "    print\n",
    "    print \"confusion for testing\\n\", conf\n",
    "    \n",
    "basic(clf)\n",
    "print X_testy.shape\n",
    "print X_trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.0\n",
      "\n",
      "training confusion:\n",
      "[[3538    0]\n",
      " [2114    0]]\n",
      "\n",
      "testing F1: 0.0\n",
      "\n",
      "confusion for testing\n",
      "[[1179    0]\n",
      " [ 705    0]]\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.0\n",
      "\n",
      "training confusion:\n",
      "[[3538    0]\n",
      " [2114    0]]\n",
      "\n",
      "testing F1: 0.0\n",
      "\n",
      "confusion for testing\n",
      "[[1179    0]\n",
      " [ 705    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.544470224285\n",
      "\n",
      "training confusion:\n",
      "[[   6 3532]\n",
      " [   2 2112]]\n",
      "\n",
      "testing F1: 0.544328300426\n",
      "\n",
      "confusion for testing\n",
      "[[   4 1175]\n",
      " [   2  703]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "basic(gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training F1: 0.00283419933869\n",
      "\n",
      "training confusion:\n",
      "[[3538    0]\n",
      " [2111    3]]\n",
      "\n",
      "testing F1: 0.002828854314\n",
      "\n",
      "confusion for testing\n",
      "[[1178    1]\n",
      " [ 704    1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
