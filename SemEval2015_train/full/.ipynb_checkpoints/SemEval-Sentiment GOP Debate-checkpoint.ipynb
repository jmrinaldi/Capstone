{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 9665 rows, 4 columns\n",
      "                   id   tweet-id             sentiment  \\\n",
      "0  264183816548130816   15140428              positive   \n",
      "1  263405084770172928  591166521              negative   \n",
      "2  262163168678248449   35266263              negative   \n",
      "3  264249301910310912   18516728              negative   \n",
      "4  262682041215234048  254373818  objective-OR-neutral   \n",
      "\n",
      "                                                text  \n",
      "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
      "1                                      Not Available  \n",
      "2                                      Not Available  \n",
      "3  Iranian general says Israel's Iron Dome can't ...  \n",
      "4                                      Not Available  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"downloaded2.tsv\", sep = '\\t')\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "print data.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              sentiment                                               text\n",
      "0              positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1              negative                                      Not Available\n",
      "2              negative                                      Not Available\n",
      "3              negative  Iranian general says Israel's Iron Dome can't ...\n",
      "4  objective-OR-neutral                                      Not Available\n"
     ]
    }
   ],
   "source": [
    "#Let's drop the id-columns, they were used to download the twitter data, with twitter API.\n",
    "df = data.drop(data.columns[[0,1]], axis=1)\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                               text\n",
      "0  positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1  negative  Iranian general says Israel's Iron Dome can't ...\n",
      "2  positive  with J Davlar 11th. Main rivals are team Polan...\n",
      "3  negative  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4  negative  They may have a SuperBowl in Dallas, but Dalla...\n",
      "Dataset has 7549 rows, 2 columns\n",
      "               sentiment                                               text\n",
      "0               positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1               negative  Iranian general says Israel's Iron Dome can't ...\n",
      "2               positive  with J Davlar 11th. Main rivals are team Polan...\n",
      "3               negative  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4               negative  They may have a SuperBowl in Dallas, but Dalla...\n",
      "5                neutral  Im bringing the monster load of candy tomorrow...\n",
      "6   objective-OR-neutral  Apple software, retail chiefs out in overhaul:...\n",
      "7               positive  @oluoch @victor_otti @kunjand I just watched i...\n",
      "8              objective  #Livewire Nadal confirmed for Mexican Open in ...\n",
      "9               positive  @MsSheLahY I didnt want to just pop up... but ...\n",
      "10               neutral  @Alyoup005 @addicted2haley hmmmm  November is ...\n",
      "11             objective  #Iran US delisting MKO from global terrorists ...\n",
      "12              positive  Good Morning Becky ! Thursday is going to be F...\n",
      "13             objective  Expect light-moderate rains over E. Visayas; C...\n",
      "14              positive  One ticket left for the @49ers game tomorrow! ...\n",
      "15              negative  AFC away fans on Saturday. All this stuff abou...\n",
      "16              negative  Why is it so hard to find the @TVGuideMagazine...\n",
      "17  objective-OR-neutral  Game 1 of the NLCS and a rematch of the NFC Ch...\n",
      "18               neutral  @TrevorJavier the heat game may cost alot more...\n",
      "19              positive  Never start working on your dreams and goals t...\n"
     ]
    }
   ],
   "source": [
    "#Now let's drop all the rows in which the tweet was no longer available.\n",
    "df = df[df.text != \"Not Available\"]\n",
    "df = df.reset_index(drop=True) #reset the index after dropping the above rows\n",
    "print df.head()\n",
    "print \"Dataset has {} rows, {} columns\".format(*df.shape)\n",
    "print df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentiment                                               text\n",
      "0           1  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
      "1           0  Iranian general says Israel's Iron Dome can't ...\n",
      "2           1  with J Davlar 11th. Main rivals are team Polan...\n",
      "3           0  Talking about ACT's &amp;&amp; SAT's, deciding...\n",
      "4           0  They may have a SuperBowl in Dallas, but Dalla...\n",
      "5           0  Im bringing the monster load of candy tomorrow...\n",
      "6           0  Apple software, retail chiefs out in overhaul:...\n",
      "7           1  @oluoch @victor_otti @kunjand I just watched i...\n",
      "8           0  #Livewire Nadal confirmed for Mexican Open in ...\n",
      "9           1  @MsSheLahY I didnt want to just pop up... but ...\n",
      "10          0  @Alyoup005 @addicted2haley hmmmm  November is ...\n",
      "11          0  #Iran US delisting MKO from global terrorists ...\n",
      "12          1  Good Morning Becky ! Thursday is going to be F...\n",
      "13          0  Expect light-moderate rains over E. Visayas; C...\n",
      "14          1  One ticket left for the @49ers game tomorrow! ...\n",
      "15          0  AFC away fans on Saturday. All this stuff abou...\n",
      "16          0  Why is it so hard to find the @TVGuideMagazine...\n",
      "17          0  Game 1 of the NLCS and a rematch of the NFC Ch...\n",
      "18          0  @TrevorJavier the heat game may cost alot more...\n",
      "19          1  Never start working on your dreams and goals t...\n"
     ]
    }
   ],
   "source": [
    "#Next we need to re-write the neutral / objective labels to all be neutral.  The organizers kept this distinction\n",
    "#for other tasks, but for this task, it's considered the same.\n",
    "# so, let's re-write all objective ->neutral, and all neutral-OR-objective --> neutral.\n",
    "\n",
    "#Since we probably will want our labels numeric (some classifiers may not like 3-way text labels),\n",
    "#we can do that all now.\n",
    "\n",
    "df = df.apply(lambda x: x.replace(['positive', 'negative', 'neutral', 'objective', 'objective-OR-neutral']\n",
    "                                  , [1, 0,0,0,0]) ,1)\n",
    "print df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples is : 7549\n",
      "There are 2820 positive tweets or 0.373559411843%\n",
      "There are 0 Negative tweets or 0.0%\n",
      "There are 4729 Neutral tweets or 0.626440588157%\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at our class distribution\n",
    "total_tweets = len(df)\n",
    "positive_tweets = sum(df.sentiment == 1)\n",
    "negative_tweets = sum(df.sentiment == -1)\n",
    "neutral_tweets = sum(df.sentiment == 0)\n",
    "\n",
    "print \"The total number of samples is : {}\".format(len(df.sentiment))\n",
    "print \"There are {} positive tweets or {}%\".format \\\n",
    "(positive_tweets, positive_tweets/float(total_tweets) )\n",
    "print \"There are {} Negative tweets or {}%\".format \\\n",
    "(negative_tweets, negative_tweets / float(total_tweets))\n",
    "print \"There are {} Neutral tweets or {}%\".format \\\n",
    "(neutral_tweets, neutral_tweets/ float(total_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$3.39!!!!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat.', ':)']\n"
     ]
    }
   ],
   "source": [
    "# Let's load the texts into lists and remove RT's and URls\n",
    "# we will build a custom function for an individual tweet, \n",
    "#and then use Pandas Dataframe.apply() to run it on all tweets.\n",
    "\n",
    "first_tweet = \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "def parse_tweet (text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "    \n",
    "parsed_tweet = parse_tweet(first_tweet)\n",
    "print parsed_tweet\n",
    "#this results in the most basic splitting operation.  However it gets us very close to what we want.\n",
    "#In the below output the only concern I have is with \"!!!!\" attached to \"$3.39\".  This is not really ideal.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours\\tourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'I', '\\\\.', 'The', '\\\\.\\\\.']\n"
     ]
    }
   ],
   "source": [
    "#stop-word removal\n",
    "with open('stopwords.txt') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "    stop_words.extend(['I', '\\.', 'The','\\.\\.']) #add upper-case I\n",
    "print stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'house', 'hit', '$339', '!', '!', '!', '!', \"'m\", 'going', 'Chapel', 'Hill', 'Sat', ':)']\n",
      "['Cool', '#cdnpoli', 'Call', 'hospital', 'Iqaluit', '&amp;', 'press', '2', 'English', 'Experience', 'aboriginal', 'language', '1st', 'choice']\n",
      "['For', 'long,', 'might', 'NJ', '?', '@blove402', 'Thursday', 'Night', '13th', 'Dec']\n",
      "['Get', 'ready', 'Wednesday', 'Drink', 'Specials', 'Wednesday', '-', '3-8pm', 'Have', 'Way', 'Margarita', 'Day', '(', 'Bar', 'Brand', 'Only)', 'DOTDOTDOT']\n"
     ]
    }
   ],
   "source": [
    "#Let's enhance the parser to deal with a few more special cases\n",
    "\n",
    "#compile regex outside of the function, because we will be running this function in a loop.\n",
    "retweets = re.compile(r'(RT ?@.*:)')   \n",
    "urls = re.compile(r'(http:.*\\b)')\n",
    "dotdotdot = re.compile(r'(\\.\\.\\.)')\n",
    "pound_question = re.compile(r'([!\\?])')\n",
    "period_dot = re.compile(r'(\\.(?!\\d))')\n",
    "stop_word = re.compile(r'\\b(?:{})\\b'.format('|'.join(stop_words)))\n",
    "dot = re.compile(r'(\\.)')\n",
    "\n",
    "regex_args = (retweets, urls, dotdotdot, pound_question, period_dot, dot, stop_word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_tweet (text , retweets, urls, dotdotdot, pound_question, period_dot,dot, stop_word):\n",
    "    text = re.sub(retweets, \"\", text) #removes RT@thisguy: or RT @thisguy:   two common Retweet bits I dont' need\n",
    "    text = re.sub(urls, \"\", text) # removes URL's\n",
    "    text = re.sub(dotdotdot, ' DOTDOTDOT ', text) #replace '...' with \"DOTDOTDOT' so i preserve the meaning in that token\n",
    "    text = re.sub(pound_question, r' \\1 ', text)  #eyes bleeding? Searches for ! ? and adds white space around them.\n",
    "    text = re.sub(period_dot, r' \\1 ', text) #more blood.  searched for '.' but looks ahead for digits. will not break 3.39\n",
    "    text = re.sub(dot, \"\", text) # remove all '.'\n",
    "    text = re.sub(stop_word, \"\", text) #removes stop words.\n",
    "    \n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "############\n",
    "#Test cases#\n",
    "############\n",
    "first_tweet = \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "RT_tweet_1 = \"Cool #cdnpoli RT@angelpike: Call the hospital in Iqaluit &amp; press 2 for English. \\\n",
    "Experience an aboriginal language as 1st choice\"\n",
    "RT_tweet_2 = \"For how long, i might be in NJ then?RT @FoolishInApril: @blove402 Thursday Night the 13th of Dec.\"\n",
    "URL_tweet = \"Get ready for our Wednesday Drink Specials Wednesday - 3-8pm Have it your Way Margarita Day \\\n",
    "( Bar Brand Only)... http://t.co/ml806WRT\"\n",
    "\n",
    "test1 = parse_tweet(first_tweet, *regex_args)\n",
    "test2 = parse_tweet(RT_tweet_1, *regex_args)\n",
    "test3 = parse_tweet(RT_tweet_2, *regex_args)\n",
    "test4 = parse_tweet(URL_tweet, *regex_args)\n",
    "print test1\n",
    "print test2\n",
    "print test3\n",
    "print test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                               text\n",
      "0          1  [Gas, house, hit, $339, !, !, !, !, 'm, going,...\n",
      "1          0  [Iranian, general, says, Israel's, Iron, Dome,...\n",
      "2          1  [J, Davlar, 11th, Main, rivals, team, Poland, ...\n",
      "3          0  [Talking, ACT's, &amp;&amp;, SAT's,, deciding,...\n",
      "4          0  [They, may, SuperBowl, Dallas,, Dallas, ain't,...\n",
      "(7549, 2)\n"
     ]
    }
   ],
   "source": [
    "# ok, now that we have rough parsing, lets parse them all!\n",
    "df.text = df.text.apply(lambda x: parse_tweet(x,*regex_args))\n",
    "print df.head()\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop some tweets that got parsed to zero\n",
    "df = df[df['text'].map(len) >= 1]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training tweets:  5649\n",
      "size of testing tweets:  1883\n"
     ]
    }
   ],
   "source": [
    "# now that we have all the tweets parsed, we actually want to split into our training / testing sets. \n",
    "# This is because n-gram analysis (which comes next), should not be done on the testing data!  \n",
    "# The n-gram analysis should on be on training data.  \n",
    "\n",
    "# TODO try to implement n-gram analysis with cross validation, for now I'll use a hold-out testing set\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#Let's split up the labels from the training data\n",
    "\n",
    "X_all = df['text']\n",
    "y_all = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "        X_all, y_all, test_size=0.25, stratify = y_all)\n",
    "\n",
    "print \"size of training tweets: \", len(X_train)\n",
    "print \"size of testing tweets: \", len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  sentiment\n",
      "4253  [BR, wednesday,, Natchitoches, thursday,, Fort...          0\n",
      "2985  [BOZO, PORNO, CIRCUS, EBM, Dollz, Live, Octobe...          0\n",
      "143   [Wish, Maddie, luck, next, saturday,, solo, au...          1\n",
      "6631  [Just, watched, Rhonda, Rousey, v, Meisha, Tat...          0\n",
      "4371  [Giants, 3rd-round, pick, failed, drug, test, ...          0\n",
      "4253    [BR, wednesday,, Natchitoches, thursday,, Fort...\n",
      "2985    [BOZO, PORNO, CIRCUS, EBM, Dollz, Live, Octobe...\n",
      "143     [Wish, Maddie, luck, next, saturday,, solo, au...\n",
      "6631    [Just, watched, Rhonda, Rousey, v, Meisha, Tat...\n",
      "4371    [Giants, 3rd-round, pick, failed, drug, test, ...\n",
      "7038    [Happy, Birthday, Wilt, Chamberlain, !, He, ma...\n",
      "175     [Aww, love, Austin, Rivers, happy, see, get, s...\n",
      "3869    [@The_Outfits, come, party, us, tomorrow, nigh...\n",
      "7512    [Thanks, @us1035, Kim, &, going, Rascal, Flatt...\n",
      "2747    [@HeartThrobEsco, oh, shit, just, reminded, wo...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# tricked you! we need to merge the labels and parsed tweets for doing our n-gram analysis.\n",
    "# This is because we will build n-gram models for each class, therefore we need to select only\n",
    "# those tweets that are positive / negative for the two n-gram tables.\n",
    "\n",
    "# Let's re-merge the labels into the training data order to do n-gram analysis\n",
    "XyN_gram = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "print XyN_gram.head()\n",
    "print XyN_gram.text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(\"'m\",): 1, ('Hill',): 1, ('!',): 4, ('$339',): 1, ('Sat',): 1, ('house',): 1, ('hit',): 1, ('Chapel',): 1, ('Gas',): 1, ('going',): 1}\n",
      " \n",
      "{('going', 'Chapel'): 1, ('!', '!'): 3, ('!', \"'m\"): 1, (\"'m\", 'going'): 1, ('hit', '$339'): 1, ('Hill', 'Sat'): 1, ('$339', '!'): 1, ('house', 'hit'): 1, ('Chapel', 'Hill'): 1, ('Gas', 'house'): 1}\n",
      " \n",
      "{('!', \"'m\", 'going'): 1, ('Chapel', 'Hill', 'Sat'): 1, ('house', 'hit', '$339'): 1, ('!', '!', \"'m\"): 1, (\"'m\", 'going', 'Chapel'): 1, ('hit', '$339', '!'): 1, ('Gas', 'house', 'hit'): 1, ('!', '!', '!'): 2, ('$339', '!', '!'): 1, ('going', 'Chapel', 'Hill'): 1}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Let's start by developing a function that will take a parsed tweet and output grams of any size\n",
    "\n",
    "uni_gram_map = {}\n",
    "bi_gram_map = {}\n",
    "tri_gram_map = {}\n",
    "\n",
    "def nGram_counter (parsed_tweet, distance_to_cover, gram_map):\n",
    "    for_loop_range = range(len(parsed_tweet) - distance_to_cover)    \n",
    "    for i in for_loop_range:\n",
    "        gram = tuple(parsed_tweet[i:i+distance_to_cover])\n",
    "        if gram in gram_map:\n",
    "            gram_map[gram] += 1\n",
    "        else:\n",
    "            gram_map[gram] = 1\n",
    "\n",
    "nGram_counter(test1, 3, tri_gram_map)\n",
    "nGram_counter(test1, 2, bi_gram_map)\n",
    "nGram_counter(test1, 1, uni_gram_map)\n",
    "\n",
    "#Our output should be a dictionaries of all possible tri-grams, bi-grams and unigrams of the tweet\n",
    "#\"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"\n",
    "\n",
    "# if a particular gram exists more than once in a tweet, the counter should have incremented.  We see an example of this\n",
    "# with the token \"!!!!\" which is parsed into \"!!\" 3 times. and \"!!!\" twice. (it overlaps).\n",
    "\n",
    "\n",
    "# check our output.\n",
    "print uni_gram_map\n",
    "print \" \"\n",
    "print bi_gram_map\n",
    "print \" \"\n",
    "print tri_gram_map\n",
    "print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unigrams for Positive Tweets : 9964\n",
      "\n",
      "Total Bi-grams for Positive Tweets: 23071\n",
      "\n",
      "Total Tri-grams for Positive Tweets: 23593\n",
      "\n",
      "Most popular Positive Uni-grams : [(('!',), 1470), (('tomorrow',), 318), (('DOTDOTDOT',), 211), (('?',), 187), (('will',), 157), (('see',), 156), (('night',), 155), (('day',), 144), (('may',), 143), ((\"'m\",), 141), ((\"'s\",), 138), (('tonight',), 128), (('just',), 126), (('-',), 124), (('going',), 119), (('good',), 109), (('&amp;',), 108), (('time',), 107), (('love',), 104), (('1st',), 102), (('game',), 97), (('like',), 96), ((',',), 96), (('get',), 94), (('wait',), 89), (('Friday',), 89), (('Sunday',), 89), (('one',), 84), (('Saturday',), 84), (('today',), 79)]\n",
      "\n",
      "Most Popular Positive  Bi-gams : [(('!', '!'), 412), ((\"Can't\", 'wait'), 37), (('tomorrow', '!'), 37), (('tonight', '!'), 29), (('tomorrow', 'night'), 28), ((\"'m\", 'going'), 24), (('Looking', 'forward'), 22), (('wait', 'see'), 21), (('!', \"'m\"), 20), (('!', 'We'), 17), (('night', '!'), 17), (('!', ':)'), 15), (('Good', 'luck'), 15), (('last', 'night'), 15), (('Day', '!'), 14), (('!', 'Hope'), 14), (('Happy', 'Birthday'), 14), (('?', '!'), 14), (('?', '?'), 13), (('Friday', 'night'), 13), (('1st', 'time'), 13), (('Friday', '!'), 13), ((\"'m\", 'gonna'), 12), (('!', 'Come'), 12), (('Good', 'morning'), 11), (('Sunday', '!'), 11), (('day', 'tomorrow'), 11), (('go', 'see'), 11), ((\"Don't\", 'miss'), 11), (('Sarah', 'G'), 10)]\n",
      "\n",
      "Most popular Positive Tri-grams : [(('!', '!', '!'), 166), ((\"Can't\", 'wait', 'see'), 10), (('!', \"Can't\", 'wait'), 8), (('!', 'Hope', 'see'), 7), (('!', \"'m\", 'going'), 6), (('Monday', 'Night', 'Football'), 6), (('World', 'Book', 'Day'), 6), (('tonight', '!', '!'), 6), (('night', '!', '!'), 6), ((\"Can't\", 'wait', 'tomorrow'), 5), (('elected', 'President', 'glorious'), 5), (('President', 'glorious', '25th'), 5), (('25th', 'Jan', 'Revolution'), 5), (('tomorrow', 'night', '!'), 5), (('glorious', '25th', 'Jan'), 5), (('Kung', 'Fu', 'Panda'), 5), (('Fu', 'Panda', '2'), 5), (('Morsi', 'announced', \"Egypt's\"), 5), (('tomorrow', '!', \"'m\"), 4), (('tomorrow', '!', '!'), 4), (('National', 'Bieber', 'Day'), 4), (('What', 'Makes', 'You'), 4), (('Mohamed', 'Morsi', 'announced'), 4), (('!', \"Don't\", 'miss'), 4), (('Happy', 'Valentines', 'Day'), 4), (('Sarah', 'G', 'Live'), 4), (('Teen', 'Choice', 'Awards'), 4), (('!', '!', \"'m\"), 4), (('see', 'Rise', 'Planet'), 4), (('Rise', 'Planet', 'Apes'), 4)]\n"
     ]
    }
   ],
   "source": [
    "#let's now apply our n-gram counter to all the tweets of a certain class.\n",
    "# Let's make the positive n_gram map, on the training data.\n",
    "\n",
    "#for now I will merge all grams into a single map, maybe harder for stats later, but easier for coding now\n",
    "\n",
    "#setup n-gram maps.\n",
    "pos_uni_gram_map ={}\n",
    "pos_bi_gram_map = {}\n",
    "pos_tri_gram_map = {}\n",
    "\n",
    "pos_tweets = XyN_gram[XyN_gram.sentiment == 1]\n",
    "\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 1, pos_uni_gram_map), 1)\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 2, pos_bi_gram_map), 1)\n",
    "pos_tweets.apply(lambda x: nGram_counter(x.text, 3, pos_tri_gram_map), 1)\n",
    "print \"Total Unigrams for Positive Tweets : {}\".format(len(pos_uni_gram_map))\n",
    "print \n",
    "print \"Total Bi-grams for Positive Tweets: {}\".format(len(pos_bi_gram_map))\n",
    "print\n",
    "print \"Total Tri-grams for Positive Tweets: {}\".format(len(pos_tri_gram_map))\n",
    "print\n",
    "print \"Most popular Positive Uni-grams : {}\" \\\n",
    ".format(sorted(pos_uni_gram_map.items(), key=lambda x: x[1], reverse = True)[:30])\n",
    "print\n",
    "print \"Most Popular Positive  Bi-gams : {}\" \\\n",
    ".format(sorted(pos_bi_gram_map.items(), key = lambda x: x[1], reverse = True)[:30])\n",
    "print\n",
    "print \"Most popular Positive Tri-grams : {}\" \\\n",
    ".format(sorted(pos_tri_gram_map.items(), key=lambda x: x[1], reverse = True)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ok, lets use these gram maps to create some features finally.\n",
    "so what we want to do is :\n",
    "\n",
    "take each tweet.text and calculate the probability of that tweet existing as a positive tweet.  \n",
    "we can use this feature to construct our first classifier, for positive tweets.\n",
    "let's start by defining a function that calculates the probability of a tweet.  \n",
    "I will need to include smoothing, normalization and worry about over / underflow.\n",
    "actually the very first step is to transform our maps into maximum likliehood probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "[6.974473427256242e-05, 6.973014434139879e-05, 6.97350069735007e-05, 6.94999478750391e-05, 6.974473427256242e-05, 6.634599436059048e-05, 6.973987028384127e-05, 6.97423021933954e-05, 6.97350069735007e-05, 6.634599436059048e-05]\n"
     ]
    }
   ],
   "source": [
    "# maximum likliehood probabilities for positive grams.\n",
    "# we will calculate maximum likliehood with smoothing, will use simple k-smoothing, with k = 1\n",
    "\n",
    "\n",
    "def calculate_maximum_likliehood (gram_map, k_smoothing = 1, Prior_map = None):\n",
    "    MLE_estimates = {}\n",
    "    total_unique_grams = len(gram_map) # this is V for smoothing \n",
    "    total_gram_count = sum(gram_map.values())\n",
    "    \n",
    "    if Prior_map != None:\n",
    "        total_prior_gram_count = sum(Prior_map.values()) # also V for smoothing on conditioned grams\n",
    "    \n",
    "    #figure out what kind of gram-map we have\n",
    "    keys = gram_map.keys()\n",
    "    if len(keys[0]) == 1: # we have unigrams\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key]+ k_smoothing) / \\\n",
    "            float(total_unique_grams + k_smoothing * total_gram_count)\n",
    "            # above will give MLE with smoothing = 1\n",
    "                \n",
    "    elif len(keys[0]) == 2: # This means we want to condition on previous uni gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float(Prior_map[key[0],] + k_smoothing * total_prior_gram_count)\n",
    "    else: #should be 3 size, so condition on previous bi-gram\n",
    "        for key in keys:\n",
    "            MLE_estimates[key] = (gram_map[key] + k_smoothing) / \\\n",
    "            float(Prior_map[key[:2]] + k_smoothing * total_prior_gram_count)\n",
    "            \n",
    "\n",
    "    return MLE_estimates\n",
    "\n",
    "MLE_pos_uni_gram = calculate_maximum_likliehood(pos_uni_gram_map , 1)\n",
    "MLE_pos_bi_gram = calculate_maximum_likliehood(pos_bi_gram_map, 1, Prior_map=pos_uni_gram_map)\n",
    "MLE_pos_tri_gram = calculate_maximum_likliehood(pos_tri_gram_map, 1, Prior_map=pos_bi_gram_map)\n",
    "\n",
    "## sanity checks\n",
    "print len(MLE_pos_uni_gram) == len(pos_uni_gram_map)\n",
    "print len(MLE_pos_bi_gram) == len(pos_bi_gram_map)\n",
    "print len(MLE_pos_tri_gram) == len(pos_tri_gram_map)\n",
    "\n",
    "# should look reasonble?\n",
    "print MLE_pos_bi_gram.values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ok, now we have MLE for all the training data.  This is n-gram analysis on the positive data.\n",
    "NExt we need to reparse all the tweets, looking up their values in the MLE_pos gram maps.\n",
    "Take log probabilities of everything If a gram doesn't exist in the correct place, then we'll use smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.75493612276e-65\n",
      "1.67214767886e-68\n",
      "2.45920658007e-71\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def v_plus_n(grams):\n",
    "    total_unique_grams = len(grams) # this is V for smoothing \n",
    "    total_gram_count = sum(grams.values()) #this is N\n",
    "    return float(total_unique_grams + total_gram_count)\n",
    "\n",
    "\n",
    "def positive_probability_calculator (parsed_tweet, gram_size):\n",
    "    \n",
    "    if len(parsed_tweet) <1:  #this will catch any empty tweets I missed earlier.\n",
    "        return \"NaN\"\n",
    "    \n",
    "    # access the gram maps we've calculated before\n",
    "    global MLE_pos_uni_gram\n",
    "    global MLE_pos_bi_gram\n",
    "    global MLE_pos_tri_gram\n",
    "    \n",
    "    global pos_uni_gram_map\n",
    "    global pos_bi_gram_map\n",
    "    \n",
    "    uni_VplusN = v_plus_n(pos_uni_gram_map) # will use these values in smoothing\n",
    "    bi_VplusN = v_plus_n(pos_bi_gram_map)\n",
    "    tri_VplusN = v_plus_n(pos_tri_gram_map)\n",
    "        \n",
    "    # gram_map should correspond to gram_size i.E bi-grams, or tri-grams etc.\n",
    "    loop_range = range(len(parsed_tweet) - gram_size)\n",
    "    prob = 0\n",
    "    \n",
    "    if gram_size == 1: #unigrams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_uni_gram: #look up the probability value we've already calculated\n",
    "                prob += math.log(MLE_pos_uni_gram[gram])\n",
    "            else:  #it's unseen so create a new probability with k-smoothing\n",
    "                #pass # penalize it with nothing\n",
    "                prob += math.log( 1.0 / uni_VplusN )  \n",
    "    \n",
    "    if gram_size == 2: #bi-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_bi_gram:\n",
    "                prob += math.log(MLE_pos_bi_gram[gram])  #look up probability we've calculated\n",
    "            \n",
    "            else:  #condition the unseen bi-gram on the seen unigram.\n",
    "                #pass\n",
    "                if (gram[0],) in pos_uni_gram_map:\n",
    "                    prob += math.log( 1.0 / (pos_uni_gram_map[gram[0],] + len(pos_uni_gram_map)))  \n",
    "                    \n",
    "                    # so if gram = ('this','cat'), and we have never seen that before.  we are\n",
    "                    # getting a probability that is: 1 / count('this') + count(unique_single grams)\n",
    "                    #obviously close to zero.  ....\n",
    "                else: #then even the first part of this unseen bigram is not the unigram database, just do V+N\n",
    "                    prob += math.log(1.0 / bi_VplusN)\n",
    "    \n",
    "    if gram_size == 3: #tri-grams\n",
    "        for i in loop_range:\n",
    "            gram = tuple(parsed_tweet[i:i+gram_size])\n",
    "            \n",
    "            if gram in MLE_pos_tri_gram:\n",
    "                prob += math.log(MLE_pos_tri_gram[gram]) # look up prob we've already calculated\n",
    "            \n",
    "            else:\n",
    "                #pass\n",
    "                if gram[:2] in pos_bi_gram_map:\n",
    "                    prob += math.log( 1.0 / (pos_bi_gram_map[gram[:2]] + len(pos_bi_gram_map)))\n",
    "                else:\n",
    "                    prob += math.log(1.0 / tri_VplusN)\n",
    "                             \n",
    "    probability = math.exp(prob) / len(parsed_tweet) # normalize by the number of grams in the tweet.\n",
    "    return probability\n",
    "   \n",
    "\n",
    "test_tweet = ['Gas', 'by', 'my', 'house', 'hit', '$3.39', '!', '!', '!', '!', \"I'm\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', ':)']\n",
    "\n",
    "print positive_probability_calculator(test_tweet,1)\n",
    "print positive_probability_calculator(test_tweet,2)\n",
    "print positive_probability_calculator(test_tweet,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "now we want to make features using the probability calculator!! time to finally get positive probability features for all our tweets.  both training and testing need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trainy = pd.DataFrame(X_train)  #have to convert the Series into a dataframe, in order to add columns\n",
    "X_trainy['POS-uni'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x, 1),1)\n",
    "X_trainy['POS-bi'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x,2),1)\n",
    "X_trainy['POS-tri'] = X_trainy.text.apply(lambda x: positive_probability_calculator(x,3),1)\n",
    "print X_trainy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_testy = pd.DataFrame(X_test) #have to convert the Series into a dataframe, in order to add columns\n",
    "X_testy['POS-uni'] = X_testy.text.apply(lambda x: positive_probability_calculator(x, 1), 1)\n",
    "X_testy['POS-bi'] = X_testy.text.apply(lambda x: positive_probability_calculator(x,2),1)\n",
    "X_testy['POS-tri'] = X_testy.text.apply(lambda x: positive_probability_calculator(x,3),1)\n",
    "print X_testy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we can now make a classifier with these features.  This classifier will predict positive labels.  Let's try a few classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First let's drop the text tweets, they aren't helpful in actual classification\n",
    "X_trainy = X_trainy.drop(X_trainy.columns[0], axis =1)\n",
    "print X_trainy.head()\n",
    "X_testy = X_testy.drop(X_testy.columns[0], axis =1)\n",
    "print X_testy.head()\n",
    "\n",
    "#should be no reason to scale data, because we've normalized it all, it's all probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "def basic(clf):\n",
    "    clf.fit(X_trainy, y_train)\n",
    "\n",
    "    x_pred = clf.predict(X_trainy)\n",
    "    F1_train = f1_score(y_train, x_pred)\n",
    "    train_conf = confusion_matrix(y_train, x_pred)\n",
    "    \n",
    "    print \"training F1:\", F1_train\n",
    "    print\n",
    "    print \"training confusion:\\n\", train_conf\n",
    "    print\n",
    "    \n",
    "    y_pred = clf.predict(X_testy)\n",
    "    F1_score = f1_score(y_test, y_pred)\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print \"testing F1:\", F1_score\n",
    "    print\n",
    "    print \"confusion for testing\\n\", conf\n",
    "    \n",
    "basic(clf)\n",
    "print X_testy.shape\n",
    "print X_trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "basic(gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "basic(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
